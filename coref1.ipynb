{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import wikiapi\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "import requests\n",
    "from string import punctuation\n",
    "from lxml import html\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/john/diploma/ner')\n",
    "cwd = os.getcwd()\n",
    "if cwd == '/home/john/diploma/ner':\n",
    "    pass\n",
    "else:\n",
    "    os.chdir('/home/john/diploma/ner')\n",
    "import ner.NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': 'Колумб', 'len': 6, 'index': 128, 'type': 'PER'}, {'text': 'Колумб', 'len': 6, 'index': 380, 'type': 'PER'}, {'text': 'Груздев', 'len': 7, 'index': 853, 'type': 'PER'}, {'text': 'Груздев', 'len': 7, 'index': 1209, 'type': 'PER'}, {'text': 'Колумба', 'len': 7, 'index': 1311, 'type': 'PER'}, {'text': 'Груздева', 'len': 8, 'index': 1483, 'type': 'PER'}, {'text': 'Груздев', 'len': 7, 'index': 1761, 'type': 'PER'}, {'text': 'Колумб', 'len': 6, 'index': 1799, 'type': 'PER'}, {'text': 'Фоминой', 'len': 7, 'index': 1868, 'type': 'PER'}, {'text': 'Колумб', 'len': 6, 'index': 1890, 'type': 'PER'}, {'text': 'Колумб', 'len': 6, 'index': 2040, 'type': 'PER'}, {'text': 'Колумбу Катя', 'len': 12, 'index': 2797, 'type': 'PER'}, {'text': 'Яновского', 'len': 9, 'index': 2936, 'type': 'PER'}, {'text': 'Америку', 'len': 7, 'index': 3167, 'type': 'LOC'}, {'text': 'Колумб', 'len': 6, 'index': 3480, 'type': 'PER'}, {'text': 'Колумба', 'len': 7, 'index': 3550, 'type': 'PER'}]\n"
     ]
    }
   ],
   "source": [
    "with open('../coref/rucoref_texts/fiction/67_zamiatin_kolumb.txt', 'r') as file:\n",
    "    inpt = file.read()\n",
    "    out = ner.NER.process_input_(inpt)\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corefsetup:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        cwd = os.getcwd()\n",
    "        if cwd == '/home/john/diploma':\n",
    "            pass\n",
    "        else:\n",
    "            os.chdir('/home/john/diploma')\n",
    "        self.docs = pd.read_csv('coref/Documents.txt', delimiter='\\t') #init pandas df for document ids and names.\n",
    "        self.groups = pd.read_csv('coref/Groups.txt', delimiter='\\t') #init coreference groups into pandas df.\n",
    "        self.tokens = pd.read_csv('coref/Tokens.txt', delimiter='\\t') #init tokens df.\n",
    "        \n",
    "\n",
    "    def prepdf(self):\n",
    "        '''\n",
    "        Specifc to the structure of rucoref data. Creates table with doc ids and names.\n",
    "        '''          \n",
    "        self.newdf = pd.DataFrame(columns=['docid', 'docname'])\n",
    "        for path in self.docs['path']:\n",
    "            name = path.split('/')[1]\n",
    "            did = self.docs.loc[self.docs.path.values == path, 'doc_id'].values[0]\n",
    "            self.newdf = self.newdf.append(pd.Series([did, name], index=['docid', 'docname']), ignore_index=True)\n",
    "        return self.newdf\n",
    "\n",
    "    def formtoks(self, tokens, file, path):\n",
    "        '''\n",
    "        format token file for UDpipe tagging and parsing\n",
    "        '''\n",
    "        newdoc = 'newdoc id = {0}'.format(path)\n",
    "        holder = '# {0}\\n# newpar\\n'.format(newdoc, )\n",
    "        did = int(self.newdf[self.newdf['docname'] == file]['docid'])\n",
    "        doc_toks = tokens[tokens['doc_id'] == did]\n",
    "        doc_toks = doc_toks.replace('\\s+', r'\"', regex=True) #quotations are not read in properly.\n",
    "        tid = 1\n",
    "        sid = 1\n",
    "        for i, trow in doc_toks.iterrows():\n",
    "            sentid = '# sent_id = {0}\\n'.format(sid)\n",
    "            line = '{0}\\t{1}\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\t_\\n'.format(tid, trow.token)\n",
    "            if tid == 1:\n",
    "                holder += sentid\n",
    "            holder += line\n",
    "            if not trow.gram == 'SENT':\n",
    "                tid += 1\n",
    "            else:\n",
    "                tid = 1\n",
    "                sid += 1\n",
    "                holder += '\\n'\n",
    "        return holder\n",
    "               \n",
    "    \n",
    "    def create_dep(self, textp, modp, outp):\n",
    "        '''\n",
    "        For creating new dependecies trees through UDpipe. Not necessary if pre-parsed. Texts should be in\n",
    "        CoNLL-U format. Will be updated using python bindings for UDpipe.\n",
    "        \n",
    "        textp - location of folder containing texts\n",
    "        \n",
    "        modp - location of UDpipe language model. Not used in current form.\n",
    "        \n",
    "        outp - location and name of output folder. Will be created if none exists.\n",
    "        '''\n",
    "        for root, dirs, files in os.walk(textp):\n",
    "            for f in files:\n",
    "                path = os.path.join(root, f)\n",
    "                tokenized = self.formtoks(self.tokens, f, path)\n",
    "                with open ('temp.txt', 'w', encoding='UTF-8') as temp:\n",
    "                    temp.write(tokenized)\n",
    "#                 tokenized = subprocess.call('udpipe/src/udpipe --tokenize \\\n",
    "#                 udpipe/src/russian-syntagrus-ud-2.0-conll17-170315.udpipe {0} > temp.txt'.format(path), shell=True)\n",
    "                tagged = subprocess.call('udpipe/src/udpipe --tag \\\n",
    "                udpipe/src/russian-syntagrus-ud-2.0-conll17-170315.udpipe temp.txt > temp2.txt', shell=True)\n",
    "                parsed = subprocess.check_output('udpipe/src/udpipe --parse \\\n",
    "                udpipe/src/russian-syntagrus-ud-2.0-conll17-170315.udpipe temp2.txt', shell=True)\n",
    "                if not os.path.exists('{0}'.format(outp)):\n",
    "                    os.makedirs('{0}'.format(outp))\n",
    "                with open('{0}/{1}'.format(outp, f), 'w', encoding='UTF-8') as stdout:\n",
    "                    stdout.write(parsed.decode('utf-8'))\n",
    "   \n",
    "\n",
    "    def get_vars(self, srow):\n",
    "        '''\n",
    "        attain variables for mention extraction\n",
    "        '''\n",
    "        deprel = srow['deprel'] #attached to dependencies of head noun\n",
    "        skip = False\n",
    "        try:\n",
    "            srow_h = int(srow['head']) \n",
    "        except ValueError:  # can't specify quotechars when creating a df.\n",
    "            skip = True\n",
    "            srow_h = None\n",
    "        spos = srow['udpos']\n",
    "        form = srow['form']\n",
    "        return deprel, spos, form, srow_h, skip\n",
    "            \n",
    "            \n",
    "    def quotations(self, frame, dep_form, head, dep, dep_tid_list, srow):\n",
    "        '''\n",
    "        matches words in french quotes if in certain range of head and depending words.\n",
    "        Only captures short mentions, to avoid longer quotations being captured.\n",
    "        '''\n",
    "        if dep_form == \"»\" and 0 < (dep - head) < 5: #catch ending quotes \n",
    "            try:\n",
    "                start = frame[frame['form'] == \"«\"]\n",
    "                st_ind = start.index.tolist()[0]\n",
    "                if 0 < (head -  st_ind) < 3:\n",
    "                    dep_range = [x for x in range(st_ind, dep+1)]\n",
    "                    for x in dep_range:\n",
    "                        dep_tid = frame.ix[x]['tid']\n",
    "                        dep_tid_list.append(dep_tid)\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return dep_tid_list\n",
    "    \n",
    "    \n",
    "    def adj_participle(self, head, dep, dep_tid_list, srow, dep_spos):\n",
    "        '''\n",
    "        matches participles if they appear before head nouns in adjective-like position.\n",
    "        '''\n",
    "        if dep_spos in ('VERB') and (head - dep) > 0:\n",
    "            dep_tid = srow['tid']\n",
    "            dep_tid_list.append(dep_tid)\n",
    "        return dep_tid_list\n",
    "\n",
    "     \n",
    "    def string_series(self, frame, dep_form, head, dep, dep_tid_list, srow, dep_spos, dep_dep_rel, cycle, indv_mention):\n",
    "        '''\n",
    "        matches string series mentions, such as lists of objects. \n",
    "        Also matches prepositions and conjunctions inside of phrases.\n",
    "        Lastly, creates seperate column for words conected by conj deprel, for individual mentions.\n",
    "        '''\n",
    "        srow_plus_one = srow['tid'] + 1.0\n",
    "        srow_min_one = srow['tid'] - 1.0\n",
    "        dep_plus_one = frame[frame['tid'] == srow_plus_one]\n",
    "        dep_min_one = frame[frame['tid'] == srow_min_one]\n",
    "        try:\n",
    "            dep_plus_one_pos = dep_plus_one['udpos'].values[0]\n",
    "            dep_min_one_pos = dep_min_one['udpos'].values[0]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        if dep_spos in ('PROPN', 'NOUN') and dep_dep_rel in ('conj') and (dep - head) > 0:\n",
    "            dep_tid = srow['tid']\n",
    "            dep_tid_list.append(dep_tid)\n",
    "            indv_mention = True\n",
    "        elif (dep_form == ',' and (dep_plus_one['deprel'].values[0] in ('conj', 'amod', 'advmod') or dep_plus_one_pos == 'CCONJ'))\\\n",
    "        or (dep_form == ',' and dep_plus_one_pos == 'ADJ' and dep_min_one_pos == 'ADJ'):\n",
    "            dep_tid = srow['tid']\n",
    "            dep_tid_list.append(dep_tid)\n",
    "        elif cycle == False and dep_spos in ('CCONJ', 'ADP') and (dep - head) > 0: #stricter for first run through.\n",
    "            dep_tid = srow['tid']\n",
    "            dep_tid_list.append(dep_tid)\n",
    "        elif cycle == True and dep_spos in ('CCONJ', 'ADP'):\n",
    "            dep_tid = srow['tid']\n",
    "            dep_tid_list.append(dep_tid)\n",
    "        return dep_tid_list, indv_mention\n",
    "        \n",
    "    \n",
    "    def find_locs (self, sent_frame, dep_form, head, dep, dep_tid_list, srow, dep_spos, dep_dep_rel, cycle, indv_mention):\n",
    "        '''\n",
    "        initiates specific mention detection sieves.\n",
    "        '''\n",
    "        dep_tid_list = self.quotations(sent_frame, dep_form, head, dep, dep_tid_list, srow) #functions for specific parts and types of mentions\n",
    "        dep_tid_list = self.adj_participle(head, dep, dep_tid_list, srow, dep_spos)\n",
    "        dep_tid_list, indv_mention = self.string_series(sent_frame, dep_form, head, dep, dep_tid_list, srow, dep_spos, dep_dep_rel, cycle, indv_mention)   \n",
    "        return dep_tid_list, indv_mention\n",
    "    \n",
    "    \n",
    "    def cycling(self, sent_frame, dep_form, head, dep, dep_tid_list, srow, dep_spos, dep_dep_rel, cycle, indv_mention):\n",
    "        '''\n",
    "        cycle through the dependency location algorithm.\n",
    "        '''\n",
    "        dep_tid_list, indv_mention = self.find_locs(sent_frame, dep_form, head, dep, dep_tid_list, srow, dep_spos, dep_dep_rel, cycle, indv_mention)\n",
    "        dep_tid = srow['tid']\n",
    "        cur_tid = int(dep_tid)\n",
    "        cycle = True\n",
    "        if not dep_form in (',', '«', '»') and not dep_dep_rel == 'case' and dep_spos not in ('VERB'):\n",
    "            dep_tid_list.append(dep_tid)\n",
    "        return cur_tid, dep_tid_list, cycle, indv_mention\n",
    "    \n",
    "    \n",
    "    def last_comma(self, word, dep_tid_list):\n",
    "        '''\n",
    "        remove comma and tid if mention ends or starts with a comma, period, or hyphen.\n",
    "        '''\n",
    "        if word.endswith(' ,') or word.endswith(' .') or word.endswith(' -'):\n",
    "            word = word[:-2]\n",
    "            del dep_tid_list[-1]\n",
    "        elif word.endswith(',') or word.endswith('.') or word.endswith('-'):\n",
    "            word = word[:-1]\n",
    "            del dep_tid_list[-1]\n",
    "        if word.startswith(', ') or word.startswith('. ') or word.startswith('- '):\n",
    "            word = word[2:]\n",
    "            del dep_tid_list[0]\n",
    "        elif word.startswith(',') or word.startswith('.') or word.startswith('-'):\n",
    "            word = word[1:]\n",
    "            del dep_tid_list[0]\n",
    "        return word, dep_tid_list\n",
    "    \n",
    "    \n",
    "    def append_mention(self, cur_tid, dep_tid_list, dep_tid_list_indv, deps, sent_frame, i3, indv_mention):\n",
    "        '''\n",
    "        create the mention and tk_shifts from stored index values.\n",
    "        '''\n",
    "        dep_tid_list.append(cur_tid)\n",
    "        dep_tid_list = list(set(dep_tid_list))\n",
    "        dep_tid_list.sort()\n",
    "        dep_tid_list_indv.append(cur_tid)\n",
    "        dep_tid_list_indv = list(set(dep_tid_list_indv))\n",
    "        dep_tid_list_indv.sort()\n",
    "        word = ''\n",
    "        word_indv = ''\n",
    "        for x in dep_tid_list:\n",
    "            frame = sent_frame[sent_frame['tid'] == float(x)]\n",
    "            word += '{0} '.format(frame['form'].values[0])\n",
    "        word = word[:-1]\n",
    "        if indv_mention == True:  # for extracting individual mentions from series and lists.\n",
    "            for x in dep_tid_list_indv:\n",
    "                frame = sent_frame[sent_frame['tid'] == float(x)]\n",
    "                word_indv += '{0} '.format(frame['form'].values[0])\n",
    "            word_indv, dep_tid_list_indv = self.last_comma(word_indv, dep_tid_list_indv)\n",
    "            deps.ix[i3, 'part_men'] = word_indv\n",
    "            deps.ix[i3, 'part_shifts'] = \", \".join(str(int(sent_frame['shift'][sent_frame['tid'] == x])) for x in dep_tid_list_indv)\n",
    "            deps.ix[i3, 'series'] = True\n",
    "        word, dep_tid_list = self.last_comma(word, dep_tid_list)\n",
    "        deps.ix[i3, 'full_men'] = word \n",
    "        deps.ix[i3, 'tk_shifts'] = \", \".join(str(int(sent_frame['shift'][sent_frame['tid'] == x])) for x in dep_tid_list)\n",
    "        return deps\n",
    "        \n",
    "    \n",
    "    def mention_det(self, prev_sid, deps, poss):\n",
    "        '''\n",
    "        creates a full mention column and tk_shifts for every NP and specific pronouns and determiners.\n",
    "        For training this all real mentions replaced with actual mentions. This module is used for\n",
    "        actual mention extraction.\n",
    "        '''\n",
    "        sent_frame = deps[deps['sid'] == prev_sid]\n",
    "        for i3, srow in sent_frame.iterrows(): #srow is an idividual token in sentence.\n",
    "            indv_mention = False\n",
    "            dep_tid_list = []\n",
    "            dep_tid_list_indv = []\n",
    "            spos = srow['udpos']\n",
    "            slem = srow['lemma']\n",
    "            sdep = srow['deprel']\n",
    "            try:\n",
    "                if spos == 'PRON' and slem in poss: #catch incorrect parsing of его, ее, их. need sentence parsed to find.\n",
    "                    head = sent_frame['udpos'][sent_frame['tid'] == float(srow['head'])].values[0]\n",
    "                    head_rel = sent_frame['deprel'][sent_frame['tid'] == float(srow['head'])].values[0] \n",
    "                    if head == 'NOUN' and not head_rel == 'root': #if the head of pronoun is noun\n",
    "                        deps.ix[i3, 'type'] = 'poss'\n",
    "            except IndexError:\n",
    "                pass\n",
    "            if spos in ('PROPN', 'NOUN') or (spos == 'NUM' and sdep in 'nsubj' or 'root') or (spos == 'ADJ' and sdep in ('nsubj', 'nmod', 'root')):\n",
    "                cur_tid = int(srow['tid'])\n",
    "                for i4, srow2 in sent_frame.iterrows():\n",
    "                    cycle = False\n",
    "                    deprel2, spos2, form2, srow2_h, skip2 = self.get_vars(srow2) #gets vars for booleans.\n",
    "                    if skip2 == False and srow2_h == cur_tid and not srow2['type'] =='rel' and\\\n",
    "                    (deprel2 in ('nmod', 'flat:name', 'nummod:gov', 'nummod', 'amod', 'conj', 'nummod:entity', 'flat:foreign') or form2 in (',', '«', '»', '\\\"', '-')):\n",
    "                        cur_tid2, dep_tid_list, cycle, indv_mention2 = self.cycling(sent_frame, form2, i3, i4, dep_tid_list, srow2, spos2, deprel2, cycle, indv_mention)\n",
    "                        if indv_mention2 == True:\n",
    "                            indv_mention = True\n",
    "                        for i5, srow3 in sent_frame.iterrows():# second check for prepositions and adjectives. Adverbs also added at this stage.\n",
    "                            deprel3, spos3, form3, srow3_h, skip3 = self.get_vars(srow3)\n",
    "                            if skip3 == False and srow3_h == cur_tid2 and not srow3['type'] =='rel' and \\\n",
    "                            (deprel3 in ('amod', 'nmod', 'nummod', 'case', 'conj', 'advmod', 'nummod:entity', 'flat:foreign') or form3 in (',', '«', '»', '\\\"', '-') or spos3 in ('CCONJ', 'PROPN')):\n",
    "                                cur_tid3, dep_tid_list, cycle, indv_mention3 = self.cycling(sent_frame, form3, i4, i5, dep_tid_list, srow3, spos3, deprel3, cycle, indv_mention)\n",
    "                                if indv_mention3 == True:\n",
    "                                    indv_mention = True\n",
    "                                for i6, srow4 in sent_frame.iterrows(): #third check for mostly adjectives, and list series\n",
    "                                    deprel4, spos4, form4, srow4_h, skip4 = self.get_vars(srow4)\n",
    "                                    if skip4 == False and srow4_h == cur_tid3 and not srow4['type'] =='rel' and \\\n",
    "                                    (deprel4 in ('case', 'amod', 'nmod', 'nummod', 'conj', 'advmod', 'flat:foreign') or spos4 in ('CCONJ', 'PROPN') or form4 in ('«', '»', ',', '\\\"', '-')): \n",
    "                                        cur_tid4, dep_tid_list, cycle, indv_mention4 = self.cycling(sent_frame, form4, i5, i6, dep_tid_list, srow4, spos4, deprel4, cycle, indv_mention)\n",
    "                                        if indv_mention4 == True:\n",
    "                                            indv_mention = True\n",
    "                                        for i7, srow5 in sent_frame.iterrows(): #forth check for list series\n",
    "                                            deprel5, spos5, form5, srow5_h, skip5 = self.get_vars(srow4)\n",
    "                                            if skip5 == False and srow5_h == cur_tid4 and not srow5['type'] =='rel' and \\\n",
    "                                            (deprel5 in ('case', 'amod', 'nmod', 'nummod', 'conj', 'advmod', 'flat:foreign') or spos5 == 'CCONJ' or form5 in ('«', '»', ',')): \n",
    "                                                cur_tid5, dep_tid_list, cycle, indv_mention5 = self.cycling(sent_frame, form5, i6, i7, dep_tid_list, srow5, spos5, deprel5, cycle, indv_mention)\n",
    "                                                if indv_mention5 == True:\n",
    "                                                    indv_mention = True\n",
    "                if indv_mention == True: #create single mentions for parts of series lists\n",
    "                    for i4, srow2 in sent_frame.iterrows():\n",
    "                        cycle = False\n",
    "                        deprel2, spos2, form2, srow2_h, skip2 = self.get_vars(srow2) #gets vars for booleans.\n",
    "                        if skip2 == False and srow2_h == cur_tid and not srow2['type'] =='rel' and\\\n",
    "                        (deprel2 in ('nmod', 'flat:name', 'nummod:gov', 'nummod', 'amod')):\n",
    "                            cur_tid2, dep_tid_list_indv, cycle, indv_mention2 = self.cycling(sent_frame, form2, i3, i4, dep_tid_list_indv, srow2, spos2, deprel2, cycle, indv_mention)\n",
    "                deps = self.append_mention(cur_tid, dep_tid_list, dep_tid_list_indv, deps, sent_frame, i3, indv_mention)\n",
    "            elif (spos in ('DET', 'PRON') and srow['type'] in ('refl', 'pron', 'rel', 'poss')):#pronouns and adjectives as nouns.\n",
    "                dep_tid = srow['tid']\n",
    "                cur_tid = int(dep_tid)\n",
    "                dep_tid_list.append(dep_tid)\n",
    "                deps = self.append_mention(cur_tid, dep_tid_list, dep_tid_list_indv, deps, sent_frame, i3, indv_mention)\n",
    "        return deps\n",
    "\n",
    "    \n",
    "    def find_type(self, s, drow, deps, depend, docdf, ner_inds, dind, sloc):\n",
    "        '''\n",
    "        find types of NEs, and pronouns, ect. Used later for coref sieves.\n",
    "        '''\n",
    "        refl = ['себя', 'свой']\n",
    "        poss = ['мой', 'он', 'она', 'они', 'наш', 'ваш', 'твой']  \n",
    "        if drow.tid == 1: \n",
    "            prev_sid = s # first ittereates through previous sentence to create full mentions and tk_shifts.\n",
    "            if prev_sid != 0:\n",
    "                deps = self.mention_det(prev_sid, deps, poss)\n",
    "            s += 1\n",
    "        drow['sid'] = s\n",
    "        drow['length'] = docdf.get_value(sloc, 'length')\n",
    "        drow['shift'] = docdf.get_value(sloc, 'shift')\n",
    "        stop = False\n",
    "        pos = drow['udpos']\n",
    "        lem = drow['lemma']\n",
    "        if pos == 'PROPN': # this section appends NE values for all words\n",
    "            if stop == False:\n",
    "                indx = drow['shift']\n",
    "                idces = [x for x in range(indx,indx-5, -1)] + [x for x in range(indx, indx+5)] #because of indexing mismatch.\n",
    "                for ind in idces:\n",
    "                    if ind in ner_inds:\n",
    "                            drow['type'] = 'NE'\n",
    "                            typ = dind[ner_inds.index(ind)]['type']\n",
    "                            drow['ne_type'] = typ\n",
    "                            stop = True\n",
    "        if pos == 'PROPN' and not 'type' in drow: #grabs people, that NER index misses\n",
    "            drow['type'] = 'NE'      \n",
    "            if drow['morph'].split('|')[0] == 'Animacy=Anim':\n",
    "                drow['ne_type'] = 'PER'\n",
    "            else:\n",
    "                drow['ne_type'] = '_'\n",
    "            stop = True\n",
    "        else:  # sets types for other than NE's, for seperation later.\n",
    "            if pos == 'NOUN':\n",
    "                drow['type'] = 'noun'\n",
    "            elif pos == 'PRON':\n",
    "                if lem in refl:\n",
    "                    drow['type'] = 'refl'\n",
    "                elif lem == 'который':\n",
    "                    drow['type'] = 'rel'\n",
    "                else:\n",
    "                    drow['type'] = 'pron'\n",
    "            elif pos == 'DET':\n",
    "                if lem in refl:\n",
    "                    drow['type'] = 'refl'\n",
    "                elif lem in poss:\n",
    "                    drow['type'] = 'poss' \n",
    "        return s, drow, deps\n",
    "    \n",
    "    \n",
    "    def fix_deps(self):\n",
    "        '''\n",
    "        creates final version of ready-to-use dependencies for feature creation and testing, including\n",
    "        ref type and NE type determination. Also calls function for mention detection.\n",
    "        '''\n",
    "        with open('ner_dict.txt', 'r') as dic:\n",
    "            ner_dict = eval(dic.read())\n",
    "        headers = ['tid', 'form', 'lemma', 'udpos', 'xpos', 'morph', 'head', 'deprel', 'deps', 'misc']\n",
    "        for i, nrow in self.newdf.iterrows():\n",
    "            print(nrow.docname)\n",
    "            dind = ner_dict[nrow.docname]  #ner list for specific doc\n",
    "            ner_inds = [x['index'] for x in dind] #list of all inds that are NE's\n",
    "            depend = pd.read_csv('coref/new_parsed_texts/{0}'.format(nrow.docname), names=headers, \\\n",
    "                                 delimiter='\\t', comment='#', quotechar = '\"')\n",
    "            depend = depend.fillna('_')\n",
    "            deps = pd.DataFrame(columns=headers)\n",
    "            docdf = self.tokens[self.tokens['doc_id'] == nrow.docid]\n",
    "            sloc = docdf[docdf['shift'] == docdf['shift'].min()].index.tolist()[0] \n",
    "            s = 0\n",
    "            for i2, drow in depend.iterrows():\n",
    "                s, drow, deps = self.find_type(s, drow, deps, depend, docdf, ner_inds, dind, sloc) #finds types of NEs and pronouns\n",
    "                sloc += 1\n",
    "                deps = deps.append(drow, ignore_index=True)\n",
    "            deps = deps.fillna('_')\n",
    "            if not os.path.exists('coref/df_ready'):\n",
    "                os.makedirs('coref/df_ready')\n",
    "            deps.to_csv('coref/df_ready/{0}'.format(nrow.docname), sep='\\t')\n",
    "                        \n",
    "    \n",
    "    def make_ner_dict(self, textp):\n",
    "        '''\n",
    "        Print out dictionary with NE locations and attributes of each text. Prints dictionary due to memory issues.\n",
    "        '''\n",
    "        self.ner_dict = defaultdict(list)\n",
    "        cwd = os.getcwd()\n",
    "        if cwd == '/home/john/diploma/ner':\n",
    "            pass\n",
    "        else:\n",
    "            os.chdir('/home/john/diploma/ner')\n",
    "        for root, dirs, files in os.walk('../{0}'.format(textp)):\n",
    "            for f in files:\n",
    "                path = os.path.join(root, f)\n",
    "                with open('{0}'.format(path), 'r') as text:\n",
    "                    text = text.read()\n",
    "                    out = ner.NER.process_input_(text)\n",
    "                    self.ner_dict[f] = out\n",
    "        self.ner_dict = dict(self.ner_dict)\n",
    "        with open('../ner_dict.txt', 'w') as file:\n",
    "            file.write(str(self.ner_dict))\n",
    "    \n",
    "    \n",
    "    def create_mentions(self):\n",
    "        '''\n",
    "        Creates Gold mentions docs.\n",
    "        '''\n",
    "        for i, nrow in self.newdf.iterrows():\n",
    "            print(nrow.docname)\n",
    "            if not os.path.exists('coref/mentions/{0}'.format(nrow.docname)):\n",
    "                hold = pd.DataFrame()\n",
    "                deps = pd.read_csv('coref/df_ready/{0}'.format(nrow.docname), delimiter='\\t')\n",
    "                docrefs = self.groups[self.groups['doc_id'] == nrow['docid']]\n",
    "                for i, ref in docrefs.iterrows():\n",
    "                    wlist = ref['content'].split(' ')\n",
    "                    if len(wlist) > 1:\n",
    "                        found = False\n",
    "                        for sft in ref['tk_shifts'].split(','):\n",
    "                            if found == False:\n",
    "                                head_test = deps[deps['shift'] == int(sft)]\n",
    "                                udpos = head_test['udpos'].values[0]\n",
    "                                if udpos == 'PROPN' or udpos == 'NOUN':\n",
    "                                    men = head_test\n",
    "                                    found = True\n",
    "                    else:\n",
    "                        men = deps[deps['shift'] == ref['shift']]\n",
    "                    men['chain_id'] = ref['chain_id']\n",
    "                    men['tk_shifts'] = ref['tk_shifts']\n",
    "                    men['full_men'] = ref['content']\n",
    "                    hold = hold.append(men)\n",
    "                hold = hold.fillna('_')\n",
    "                if not os.path.exists('coref/mentions'):\n",
    "                    os.mkdir('coref/mentions')\n",
    "                hold.to_csv('coref/mentions/{0}'.format(nrow.docname), sep='\\t')\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    def old_morph(self, headmen, head_pos):\n",
    "        '''\n",
    "        gathers morphological info from head mention.\n",
    "        '''\n",
    "        leng = len(headmen['morph'].values[0].split('|'))\n",
    "        if leng == 4:\n",
    "            ment_anim = headmen['morph'].values[0].split('|')[0].replace('Animacy=', '').split(',')\n",
    "            ment_case = headmen['morph'].values[0].split('|')[1].replace('Case=', '') #only one possible value.\n",
    "            ment_gen = headmen['morph'].values[0].split('|')[2].replace('Gender=', '').split(',')\n",
    "            ment_num = headmen['morph'].values[0].split('|')[3].replace('Number=', '').split(',')\n",
    "        elif leng == 3:\n",
    "            ment_anim = None\n",
    "            ment_case = headmen['morph'].values[0].split('|')[0].replace('Case=', '') #only one possible value.\n",
    "            ment_gen = headmen['morph'].values[0].split('|')[1].replace('Gender=', '').split(',')\n",
    "            ment_num = headmen['morph'].values[0].split('|')[2].replace('Number=', '').split(',')\n",
    "        else:\n",
    "            ment_anim = None\n",
    "            ment_case = None\n",
    "            ment_gen = None\n",
    "            ment_num = None\n",
    "        return ment_anim, ment_case, ment_gen, ment_num\n",
    "    \n",
    "    \n",
    "    def new_morph(self, new_anim, new_gen, new_num, ment_anim, ment_gen, ment_num, mer_men):\n",
    "        '''\n",
    "        combines morphological features of mentions with head mention.\n",
    "        '''\n",
    "        if not ment_anim == None:\n",
    "            if len(mer_men['morph'].values[0].split('|')) == 4:\n",
    "                mer_anim = mer_men['morph'].values[0].split('|')[0].replace('Animacy=', '').split(', ')\n",
    "                mer_gen = mer_men['morph'].values[0].split('|')[2].replace('Gender=', '').split(', ')\n",
    "                mer_num = mer_men['morph'].values[0].split('|')[3].replace('Number=', '').split(', ')\n",
    "                for mer_a in mer_anim:\n",
    "                    if mer_a not in ment_anim: #merging morphological features, if different.\n",
    "                        new_anim.append(mer_a)\n",
    "                for mer_g in mer_gen:\n",
    "                    if mer_g not in ment_gen:\n",
    "                        new_gen.append(mer_g)\n",
    "                for mer_n in mer_num:\n",
    "                    if mer_n not in ment_num:\n",
    "                        new_num.append(mer_n)\n",
    "                no_merge = False\n",
    "            else:\n",
    "                no_merge = True\n",
    "        elif not ment_gen == None:\n",
    "            mer_gen = mer_men['morph'].values[0].split('|')[1].replace('Gender=', '').split(', ')\n",
    "            mer_num = mer_men['morph'].values[0].split('|')[2].replace('Number=', '').split(', ')\n",
    "            new_anim = None\n",
    "            for mer_g in mer_gen:\n",
    "                if mer_g not in ment_gen:\n",
    "                    new_gen.append(mer_g)\n",
    "            for mer_n in mer_num:\n",
    "                if mer_n not in ment_num:\n",
    "                    new_num.append(mer_n)\n",
    "            no_merge = False\n",
    "        else:\n",
    "            new_anim = None\n",
    "            new_gen = None\n",
    "            new_num = None\n",
    "            no_merge = True\n",
    "        return new_anim, new_gen, new_num, no_merge\n",
    "          \n",
    "\n",
    "# merge types\n",
    "        \n",
    "   \n",
    "    def morph_merge(self, headmen, merges, mens):\n",
    "        '''\n",
    "        merges morphology of mentions to head mention.\n",
    "        '''\n",
    "        head_pos = headmen['udpos']\n",
    "        ment_anim, ment_case, ment_gen, ment_num = self.old_morph(headmen, head_pos) #creates list of all morph features of head mention.\n",
    "        new_anim = []\n",
    "        new_gen = []\n",
    "        new_num = []\n",
    "        if not ment_anim == None:\n",
    "            new_anim.extend(ment_anim)\n",
    "        elif not ment_case == None:\n",
    "            new_gen.extend(ment_gen)\n",
    "            new_num.extend(ment_num)\n",
    "        for mer in merges:\n",
    "            mer_men = mens[mens['shift'] == mer]\n",
    "            new_anim, new_gen, new_num, no_merge = self.new_morph(new_anim, new_gen, new_num, ment_anim, ment_gen, ment_num, mer_men)\n",
    "        if not new_anim == None and no_merge == False:\n",
    "            new_anim = \", \".join(list(set(new_anim))) #remove repeating features.\n",
    "            new_gen = \", \".join(list(set(new_gen)))\n",
    "            new_num = \", \".join(list(set(new_num)))\n",
    "            new_mor = 'Animacy={0}|Case={1}|Gender={2}|Number={3}'.format(new_anim, ment_case, new_gen, new_num)\n",
    "        elif no_merge == False and not new_gen == None:\n",
    "            new_gen = \", \".join(list(set(new_gen)))\n",
    "            new_num = \", \".join(list(set(new_num)))\n",
    "            new_mor = 'Case={0}|Gender={1}|Number={2}'.format(ment_case, new_gen, new_num)\n",
    "        else:\n",
    "            new_mor = headmen['morph'].values[0]\n",
    "        return new_mor   \n",
    "    \n",
    "    \n",
    "    def sid_merge(self, headmen, headmen_id, merges, mens):\n",
    "        '''\n",
    "        merges sentence id info, for later calculation of distances, as program chooses closest distance\n",
    "        when considering sentence range. Creates clust_head category for referencing back to head in order to\n",
    "        gather cluster level info.\n",
    "        '''\n",
    "        head_sid = headmen['sid'].values #extract all values\n",
    "        new_sids = []\n",
    "        new_sids.extend(head_sid)\n",
    "        clust_head_of_head = mens.ix[headmen_id, 'clust_head'] # find head of head mention.\n",
    "        for mer in merges:\n",
    "            mer_men = mens[mens['shift'] == mer]\n",
    "            mer_sid = mer_men['sid'].values\n",
    "            mer_id = mer_men.index.tolist()[0] #check functioning!\n",
    "            new_sids.extend(mer_sid)\n",
    "            if not clust_head_of_head == '_': # adds head_clust of headmen to mention if it exists.\n",
    "                mens.ix[mer_id, 'clust_head'] = clust_head_of_head\n",
    "            else:\n",
    "                mens.ix[mer_id, 'clust_head'] = headmen['shift'].values[0] #shift head\n",
    "        new_sids = ', '.join(list(set([str(x) for x in new_sids])))\n",
    "        mens.ix[headmen_id, 'sid_corefs'] = new_sids\n",
    "        return mens\n",
    "            \n",
    "            \n",
    "            \n",
    "    def merge(self, mens, men_dict, manual, morph_merge, gold): # main merge function.\n",
    "        '''\n",
    "        merges data for corefering mentions(i.e. sid, shift of all), appending necessary \n",
    "        info to head mention (often most informative mention.), removes all child \n",
    "        mentions from mention list, replaces child mention pairings in candidate list with head word. \n",
    "        Because features are aggregated across all mentions in cluster, will not make impact on performance.\n",
    "        '''      \n",
    "        if manual == True:\n",
    "            for ment, merges in men_dict.items():\n",
    "                skip = False\n",
    "                headmen = mens[mens['shift'] == ment]\n",
    "                if gold == True: #to select for merging in gold model, only those that exist.\n",
    "                    indexes = []\n",
    "                    if not headmen.empty:\n",
    "                        indexes.append(ment)\n",
    "                    for mer in merges:\n",
    "                        mermen = mens[mens['shift'] == mer]\n",
    "                        if not mermen.empty:\n",
    "                            indexes.append(mer)\n",
    "                    if len(indexes) > 0:\n",
    "                        indexes.sort() # to get the most early occuring mention first in order.\n",
    "                        head_sh = indexes[0]\n",
    "                        headmen = mens[mens['shift'] == head_sh]\n",
    "                        headmen_id = headmen.index.tolist()[0]\n",
    "                        del indexes[0]\n",
    "                        merges = indexes \n",
    "                    else:\n",
    "                        skip = True\n",
    "                else:\n",
    "                    try: #because of trying to train mens and deps at same time\n",
    "                        headmen_id = headmen.index.tolist()[0]\n",
    "                    except IndexError:\n",
    "                        print('first ment error!', ment)               \n",
    "                if skip == False:\n",
    "                    mens.ix[headmen_id, 'corefs'] = ', '.join([str(x) for x in merges]) # add shifts of all corefering mentions\n",
    "                    if len(merges) > 0:\n",
    "                        if morph_merge == True:\n",
    "                            new_mor = self.morph_merge(headmen, merges, mens)\n",
    "                            mens.ix[headmen_id, 'morph'] = new_mor\n",
    "                        mens = self.sid_merge(headmen, headmen_id, merges, mens) \n",
    "                    mens = mens.fillna('_')\n",
    "            return mens\n",
    "            \n",
    "    \n",
    "    def manual_sieves(self, mens, deps, check, docname):\n",
    "        '''\n",
    "        cycles through all docs for the manual sieves and prints output to folder.\n",
    "        '''\n",
    "        manual = True\n",
    "        mentions_final = pd.DataFrame(columns = ['clust_head', 'part_men'])\n",
    "        mentions_final = self.head_match_sieve(mens, mentions_final, deps, manual, gold=True)\n",
    "        deps = self.head_match_sieve(deps, mentions_final, deps, manual, gold=False)\n",
    "        deps, mentions_final = self.discourse_sieve(mentions_final, deps, manual) #change to mentions_final in final version!!!\n",
    "        deps, mentions_final = self.rel_sieve(mentions_final, deps, manual)\n",
    "        deps, mentions_final = self.alias_sieve(mentions_final, deps, manual)\n",
    "        if check == True:\n",
    "            if not os.path.exists('coref/sieves/'):\n",
    "                os.mkdir('coref/sieves/')\n",
    "            deps.to_csv('coref/sieves/{0}'.format(docname), sep='\\t')\n",
    "            if not os.path.exists('coref/men_sieves/'):\n",
    "                os.mkdir('coref/men_sieves/')\n",
    "            mentions_final.to_csv('coref/men_sieves/{0}'.format(docname), sep='\\t')\n",
    "        \n",
    "            \n",
    "            \n",
    "#all manual sieves\n",
    "\n",
    "\n",
    "    def head_match_sieve(self, mens, mentions_final, deps, manual, gold): # need to test\n",
    "        '''\n",
    "        matches head words and merges on match. !!!If there is a head noun than pair that instead.\n",
    "        if gender same on dep.\n",
    "        '''\n",
    "        morph_merge = True\n",
    "        men_dict = defaultdict(list)\n",
    "        men_past = []\n",
    "        for i2, mention1 in mens.iterrows():\n",
    "            skip = False\n",
    "            men1_sh = mention1['shift']\n",
    "            sent1 = deps[deps['sid'] == mention1['sid']]\n",
    "            for i3, check in sent1.iterrows():\n",
    "                if men1_sh in check['tk_shifts'].split(',') and not check['series'] == 'True' and skip == False:\n",
    "                    mention1 = check\n",
    "                    skip = True #only once, first mention found will always be longest\n",
    "            men1_pos = mention1['udpos']\n",
    "            men1_dep = mention1['deprel']\n",
    "            men_lem = mention1['lemma']\n",
    "            if men1_pos in ('PROPN') and men1_sh not in men_past: #avoid repeats\n",
    "                for i4, mention2 in mens.iterrows():\n",
    "                    skip2 = False\n",
    "                    men2_sh = mention2['shift']\n",
    "                    sent2 = deps[deps['sid'] == mention2['sid']]\n",
    "                    men_lem2 = mention2['lemma']\n",
    "                    for i4, check2 in sent1.iterrows():\n",
    "                        if men2_sh in check2['tk_shifts'].split(',') and not check2['series'] == 'True'\\\n",
    "                        and skip2 == False:\n",
    "                            mention2 = check2\n",
    "                            skip2 = True\n",
    "                    if men_lem == men_lem2 and not men1_sh == men2_sh and men2_sh not in men_past:\n",
    "                        men_dict[mention1['shift']].append(mention2['shift'])\n",
    "                        templist = [mention1['shift'], mention2['shift']]\n",
    "                        men_past.extend(templist)\n",
    "                        men_past = list(set(men_past)) #remove copies\n",
    "        mentions_final = self.merge(mens, men_dict, manual, morph_merge, gold) #merge morph feats\n",
    "        return mentions_final\n",
    "    \n",
    "    \n",
    "    def find_quote(self, token, sent, deps, sid, mentions_final, prev_nar, found, new_par, first):\n",
    "        '''\n",
    "        Finds the narrator of discourse. Also marks new_par false only once new narrator is found to avoid\n",
    "        the second boolean comparison being triggered early. Assumes paragraph seperation for new dialogue\n",
    "        narrators.\n",
    "        '''\n",
    "        for si2, token2 in sent.iterrows():\n",
    "            try:\n",
    "                tok2_head = float(token2['head'])\n",
    "                if found == False:\n",
    "                    if token2['deprel'] == 'nsubj': #if there is a nsubj then it will be narrator, if head root.\n",
    "                        rootv = sent[sent['tid'] == tok2_head] # get head of nsubj noun.\n",
    "                        rootv_dep =  rootv['deprel'].values[0] \n",
    "                        if rootv_dep == 'root': #check if head verb of nsubj is root.\n",
    "                            nar = token2['shift']\n",
    "                            deps.ix[deps['sid'] == sid, 'nar'] = nar # append to deps and mentions\n",
    "                            mentions_final.ix[mentions_final['sid'] == sid, 'nar'] = nar\n",
    "                            if first == True and sid > 1: #if symbol first and behind it is a connected clause then \n",
    "                                sent_min_one = deps[deps['sid'] == (sid - 1)] # that clasue also gets identical nar.\n",
    "                                sent_min_one_end = sent_min_one.ix[sent_min_one['tid'].idxmax()] #extend to go until hits par.\n",
    "                                sent_min_one_end_sh = sent_min_one_end['shift']\n",
    "                                sent_start_sh = token['shift']\n",
    "                                sent_dif = sent_start_sh - sent_min_one_end_sh #check shift distance btw end of current sent and next\n",
    "                                sent_min_one_nar = sent_min_one['nar']\n",
    "                                if sent_dif < 3 and not sent_min_one_nar == '_':\n",
    "                                    deps.ix[deps.sid == sid, 'nar'] = sent_min_one_nar\n",
    "                                    mentions_final.ix[mentions_final.sid == sid, 'nar'] = sent_min_one_nar\n",
    "                                elif sent_dif < 3 and sent_min_one_nar == '_':   \n",
    "                                    deps.ix[deps.sid == (sid - 1), 'nar'] = nar # append to deps and mentions\n",
    "                                    mentions_final.ix[mentions_final.sid == (sid - 1), 'nar'] = nar\n",
    "                            found = True\n",
    "                            new_par = False #set to false\n",
    "                            prev_nar = nar\n",
    "                        else:\n",
    "                            nar = '_'\n",
    "                            prev_nar = nar\n",
    "                            deps.ix[deps.sid == sid, 'nar'] = nar # if first mention matched. Need to fix!!!\n",
    "                            mentions_final.ix[mentions_final.sid == sid, 'nar'] = nar\n",
    "                    else:\n",
    "                        nar = '_'\n",
    "                        prev_nar = '_'\n",
    "                        deps.ix[deps.sid == sid, 'nar'] = nar\n",
    "                        mentions_final.ix[mentions_final.sid == sid, 'nar'] = nar\n",
    "            except ValueError:\n",
    "                pass\n",
    "        return deps, mentions_final, nar, new_par, found\n",
    "                            \n",
    "                                \n",
    "    \n",
    "    def find_dialogue(self, sid, sent, deps, mentions_final, prev_nar, new_par, sent_end, shift_dif_start):\n",
    "        '''\n",
    "        Finds sentence with dialogue. If there is indication of non dialouge in same sentence\n",
    "        takes the subject of the root verb as the narrator.\n",
    "        '''\n",
    "        found = False\n",
    "        for si, token in sent.iterrows():\n",
    "            tok_tid = token['tid']\n",
    "            tok_form = token['form']\n",
    "            sent_end_tid = sent_end['tid']\n",
    "            if tok_tid == 1.0 and tok_form in ('-', '\\\"') and shift_dif_start < 3 and found == False:\n",
    "                first = True\n",
    "                deps, mentions_final, nar, new_par, found = self.find_quote(token, sent, deps, sid, mentions_final, prev_nar, found, new_par, first)                \n",
    "            elif not tok_tid in (1.0, sent_end_tid) and tok_form in ('-', '\\\"') and found == False: #check all other than first token\n",
    "                tok_min_one_pos = sent.ix[si-1]['udpos']\n",
    "                tok_plus_one_pos = sent.ix[si+1]['udpos']\n",
    "                first = False\n",
    "                if tok_form == '-' and tok_min_one_pos == 'PUNCT': #check if there is a shift from dialogue to text in same sentence.\n",
    "                    deps, mentions_final, nar, new_par, found = self.find_quote(token, sent, deps, sid, mentions_final, prev_nar, found, new_par, first)\n",
    "                elif tok_form == '\\\"' and (tok_min_one_pos or tok_plus_one_pos) == 'PUNCT': #check before and after for punctuation.\n",
    "                    deps, mentions_final, nar, new_par, found = self.find_quote(token, sent, deps, sid, mentions_final, prev_nar, found, new_par, first)\n",
    "        if found == False and new_par == False: #if nothing is found in sentence, but their was no indication of a new line.\n",
    "            nar = prev_nar\n",
    "            deps.ix[deps.sid == sid, 'nar'] = nar # append to deps and mentions\n",
    "            mentions_final.ix[mentions_final.sid == sid, 'nar'] = nar\n",
    "            found = True\n",
    "        elif found == False:\n",
    "            nar = prev_nar\n",
    "            deps.ix[deps.sid == sid, 'nar'] = '_'\n",
    "            mentions_final.ix[mentions_final.sid == sid, 'nar'] = '_'\n",
    "        return deps, mentions_final, nar, new_par\n",
    "                \n",
    "                \n",
    "    def find_new_par(self, deps, sent, sent_end, new_par, sid, prev_nar):\n",
    "        '''\n",
    "        Set new_par to true if new paragraph or line break found.\n",
    "        '''\n",
    "        sent2 = deps[deps['sid'] == (sid + 1)]\n",
    "        sent2_start = sent2[sent2['tid'] == 1.0]\n",
    "        sent2_start_sh = sent2_start['shift'].values[0]\n",
    "        sent1_end_sh = sent_end['shift']\n",
    "        sent_dif = sent2_start_sh - sent1_end_sh #check shift distance btw end of current sent and next\n",
    "        if sent_dif > 3:\n",
    "            new_par = True\n",
    "            prev_nar = '_' #resets prev_nar after new paragraph.\n",
    "        return new_par, prev_nar\n",
    "    \n",
    "    \n",
    "    def pro_disc(self, mens, manual, gold):\n",
    "        '''\n",
    "        merge pronouns in first person, where the speaker is identified.\n",
    "        '''\n",
    "        morph_merge = False\n",
    "        men_dict = defaultdict(list)\n",
    "        men_past = []\n",
    "        for i, mention in mens.iterrows():\n",
    "            men1_sh = mention['shift']\n",
    "            if mention['lemma'] in ('я', 'мой') and men1_sh not in men_past and not mention['nar'] == '_': #avoid repeats\n",
    "                men_dict[mention['nar']].append(men1_sh)\n",
    "                templist = [men1_sh]\n",
    "                men_past.extend(templist)\n",
    "                men_past = list(set(men_past)) #remove copies  \n",
    "        mentions_final = self.merge(mens, men_dict, manual, morph_merge, gold)\n",
    "        return mentions_final \n",
    "        \n",
    "    \n",
    "    def discourse_sieve(self, mentions_final, deps, manual):\n",
    "        '''\n",
    "        Finds speaker for mentions found in discourse models. Calls pro_discourse\n",
    "        in order to resolve pronouns in discourse. (1st and 2nd singular and plural)\n",
    "        '''\n",
    "        sids = list(deps['sid'].unique())\n",
    "        if '_' in sids: sids.remove('_')\n",
    "        prev_nar = '_' \n",
    "        new_par = True\n",
    "        last_sent_id = deps.ix[deps['sid'].idxmax()]\n",
    "        last_sent = last_sent_id['sid']\n",
    "        for sid in sids:\n",
    "            sent = deps[deps['sid'] == sid]\n",
    "            if len(sent.index) > 2:\n",
    "                sent_end = sent.ix[sent['tid'].idxmax()]\n",
    "                start = sent[sent['tid'] == 1.0]\n",
    "                start_form = start['form'].values[0]\n",
    "                start_plus_one = sent['tid'][sent['tid'] == 2.0].values[0]      \n",
    "                shift_dif_start =  start_plus_one - 1 #get space difference\n",
    "                deps, mentions_final, prev_nar, new_par = self.find_dialogue(sid, sent, deps, mentions_final, prev_nar, new_par, sent_end, shift_dif_start) # for dialouge inside sentences.\n",
    "                if not sid == last_sent:\n",
    "                    new_par, prev_nar = self.find_new_par(deps, sent, sent_end, new_par, sid, prev_nar) #analyze if new paragraph.\n",
    "        deps = deps.fillna('_')\n",
    "        mentions_final = mentions_final.fillna('_')\n",
    "        mentions_final = self.pro_disc(mentions_final, manual, gold=True)\n",
    "        deps = self.pro_disc(deps, manual, gold=False) # for ease of use currently. DELETE LATER!\n",
    "        return deps, mentions_final     \n",
    "\n",
    "\n",
    "    def rel_sieve(self, mentions_final, deps, manual): #excluded for now. Too many errors. Better to include in ML model.\n",
    "        '''\n",
    "        sieve for labeling all possible morph features of relative pronouns. (aka который)\n",
    "        Info will be used later in ML pronoun resolution.\n",
    "        '''\n",
    "        men_dict = defaultdict(list)\n",
    "        morph_merge = True\n",
    "        sids = list(deps['sid'].unique()) #possibly combine later so not to reiterate sents twice.\n",
    "        if '_' in sids: sids.remove('_')\n",
    "        masc = ['ый', 'ому', 'ом', 'ого']\n",
    "        fem = ['ой', 'ую', 'ая']\n",
    "        plur = ['ые', 'ых', 'ыми']\n",
    "        for sid in sids:\n",
    "            stop = False\n",
    "            sent = deps[deps['sid'] == sid]\n",
    "            for i, token in sent.iterrows():\n",
    "                if stop == False:\n",
    "                    t_lem = token['lemma']\n",
    "                    t_shift = token['shift']\n",
    "                    if t_lem == 'который':\n",
    "                        t_form = token['form']\n",
    "                        prev_morph = token['morph']\n",
    "                        for x in masc: #determine morph info for rel mention\n",
    "                            if t_form.endswith(x):\n",
    "                                t_gen = 'Gender=Masc, Neut'\n",
    "                                t_num = 'Number=Sing'\n",
    "                        for x in fem:\n",
    "                            if t_form.endswith(x):\n",
    "                                t_gen = 'Gender=Fem'\n",
    "                                t_num = 'Number=Sing'\n",
    "                        for x in plur:\n",
    "                            if t_form.endswith(x):\n",
    "                                t_gen = 'Gender=Masc, Neut, Fem'\n",
    "                                t_num = 'Number=Plur'\n",
    "                        if t_form.endswith('ым'):\n",
    "                            if token['morph'] == 'Case=Ins':\n",
    "                                t_gen = 'Gender=Masc, Neut'\n",
    "                                t_num = 'Number=Sing'\n",
    "                            else:\n",
    "                                t_gen = 'Gender=Masc, Neut, Fem'\n",
    "                                t_num = 'Number=Plur'\n",
    "                        elif t_form.endswith('ое'):\n",
    "                            t_gen = 'Gender=Neut'\n",
    "                            t_num = 'Number=Sing'\n",
    "                        n_morph = '{0}|{1}|{2}'.format(prev_morph, t_gen, t_num)\n",
    "                        deps.ix[i, 'morph'] = n_morph\n",
    "                        try:\n",
    "                            men_id = mentions_final[mentions_final['shift'] == t_shift].index.tolist()[0]\n",
    "                            mentions_final.ix[men_id, 'morph'] = n_morph\n",
    "                            men_dict[t_shift].append(t_shift)\n",
    "                        except:\n",
    "                            pass\n",
    "#         deps = self.merge(deps, men_dict, manual, morph_merge, gold=False)  #excess head ref info to self won't matter.\n",
    "#         mentions_final = self.merge(mentions_final, men_dict, manual, morph_merge, gold=True)                                 \n",
    "        return deps, mentions_final\n",
    "    \n",
    "    \n",
    "    def make_alias_dict(self, i, alias, deps, alias_dict, men_past):\n",
    "        '''\n",
    "        make_dictionary for merging\n",
    "        '''\n",
    "        word_sh = deps.ix[i, 'shift']\n",
    "        for i2, ment2 in deps.iterrows():\n",
    "            ment2_sh = ment2['shift']\n",
    "            ment2_anim = ment2['morph'].split('|')[0]\n",
    "            if (ment2['full_men'] in alias or 'part_men' in ment2 and ment2['part_men'] in alias or ment2['lemma'] in alias) and \\\n",
    "            ment2_sh not in men_past and word_sh not in men_past and not ment2_sh == word_sh and \\\n",
    "            not ment2_anim == 'Animacy=Anim': #also check for animacy. Should only be handled in head_match sieve\n",
    "                sent = ment2[ment2['sid'] == ment2['sid']]\n",
    "                ment2_hid = float(ment2['head'])\n",
    "                try: #because of quotations\n",
    "                    ment_head = sent[sent['tid'] == ment2_hid]\n",
    "                    if ment_head['udpos'] not in ('NOUN', 'PROPN'):\n",
    "                        alias_dict[word_sh].append(ment2_sh)\n",
    "                        men_past.append(ment2_sh)\n",
    "                    else:\n",
    "                        pass\n",
    "                except TypeError:\n",
    "                    pass\n",
    "        return alias_dict, men_past\n",
    "             \n",
    "        \n",
    "    def remove_accents(self, word):\n",
    "        '''\n",
    "        remove pronunciation accents on wikpedia\n",
    "        '''\n",
    "        replacements = [('е́', 'e'), ('и́', 'и'), ('а́', 'а'), ('о́', 'о'), ('у́', 'у'), ('ы́', 'ы'), ('э́', 'э'), \\\n",
    "                    ('ю́', 'ю'), ('я́', 'я'), ('А́', 'А'), ('Е́','Е'), ('И́','И'), ('О́','О'), ('У́','У'), ('Ы́','Ы'), \\\n",
    "                    ('Э́','Э'), ('Ю́','Ю'), ('Я́','Я')]\n",
    "        for a, b in replacements:\n",
    "            word = word.replace(a, b)\n",
    "        return word    \n",
    "    \n",
    "    \n",
    "    def wiki_alias(self, word, r, punct):\n",
    "        '''\n",
    "        find aiases from wikipedia\n",
    "        '''\n",
    "        tree = html.fromstring(r.text)\n",
    "        bolds = tree.xpath(\".//div[@class='mw-content-ltr']/p[1]/b/text()\") #bold words in summary often acronyms.\n",
    "        if len(bolds) < 1:\n",
    "            bolds = tree.xpath(\"(.//div[@class='mw-content-ltr']//p[1])[1]/b/text()\")\n",
    "        appos = tree.xpath(\".//div[@class='mw-content-ltr']/p[1]/text()|.//div[@class='mw-content-ltr']/p[1]/a/text()\")\n",
    "        if len(appos) < 1:\n",
    "            appos = tree.xpath(\"(.//div[@class='mw-content-ltr']//p[1])[1]/text()| \\\n",
    "            (.//div[@class='mw-content-ltr']//p[1])[1]/a/text()\")\n",
    "        stop = False\n",
    "        for w in appos: #capture first link-word group after — which is indicative of a noun and apposition. Can improve.\n",
    "            if '\\xa0—' in w and stop == False:\n",
    "                i = appos.index(w)\n",
    "                i2 = i + 1\n",
    "                try:\n",
    "                    bolds.append(appos[i2])\n",
    "                except IndexError:\n",
    "                    pass\n",
    "                stop = True\n",
    "        for p in punct:\n",
    "            for w in bolds:\n",
    "                if '\\xa0— ' in w:\n",
    "                    del bolds[bolds.index(w)]\n",
    "                if p in w:\n",
    "                    words = w.split(p)\n",
    "                    i = bolds.index(w)\n",
    "                    del bolds[i]\n",
    "                    for wd in words:\n",
    "                        if not wd == '':\n",
    "                            bolds.append(wd)\n",
    "        new_words = [self.remove_accents(x) for x in bolds]\n",
    "        if ' ' in new_words:\n",
    "            del new_words[new_words.index(' ')]\n",
    "        if 'нет статьи' in new_words:\n",
    "            new_words = []\n",
    "        return new_words\n",
    "            \n",
    "           \n",
    "    def make_alias(self, word, punct):\n",
    "        '''\n",
    "        makes potential acronym by taking words in mention and combining all first letters.\n",
    "        '''\n",
    "        stop = False\n",
    "        for p in punct:\n",
    "            if p in word:\n",
    "                n_word = ''\n",
    "                stop = True\n",
    "        if ' ' in word and stop == False:\n",
    "            words = word.split(' ') #removing spaces\n",
    "            n_word = []\n",
    "            if ' ' in words:\n",
    "                del words[words.index(' ')]\n",
    "            for w in words:\n",
    "                if ' ' in w:\n",
    "                    w = w.replace(' ', '')\n",
    "                n_word += w[0]\n",
    "            n_word = \"\".join(n_word)\n",
    "        else:\n",
    "            n_word = ''\n",
    "        return n_word\n",
    "            \n",
    "            \n",
    "    def alias_sieve(self, mens, deps, manual):\n",
    "        '''\n",
    "        Finds acronyms and other potential names for mentions.\n",
    "        '''\n",
    "        morph_merge = True\n",
    "        alias_dict = defaultdict(list)\n",
    "        men_past = []\n",
    "        for i, ment in deps.iterrows():\n",
    "            alias_sh = []\n",
    "            ment_pos = ment['udpos']\n",
    "            ment_anim = ment['morph'].split('|')[0]\n",
    "            if ment_pos in ('PROPN') and not ment_anim == 'Animacy=Anim': #so that alias doesn\n",
    "                if 'series' in ment and ment['series'] == True:\n",
    "                    word = ment['part_men'] \n",
    "                else:\n",
    "                    word = ment['full_men']\n",
    "                r = requests.get('https://ru.wikipedia.org/wiki/{0}'.format(word))\n",
    "                status = r.status_code\n",
    "                punct = punctuation + '«»'\n",
    "                alias = []\n",
    "                if status == 200:\n",
    "                    alias.extend(self.wiki_alias(word, r, punct))\n",
    "                else:\n",
    "                    word = word.upper()\n",
    "                    alias.extend(self.wiki_alias(word, r, punct))\n",
    "                alias.append(self.make_alias(word, punct))\n",
    "                if '' in alias:\n",
    "                    del alias[alias.index('')]\n",
    "                alias = list(set(alias)) # remove repeats\n",
    "                if len(alias) > 0:\n",
    "                    alias_dict, men_past = self.make_alias_dict(i, alias, deps, alias_dict, men_past)\n",
    "        if len(alias_dict) > 0:\n",
    "            deps = self.merge(deps, alias_dict, manual, morph_merge, gold=False)\n",
    "            mentions_final = self.merge(mens, alias_dict, manual, morph_merge, gold=True)\n",
    "        else:\n",
    "            mentions_final = mens\n",
    "        return deps, mentions_final\n",
    "            \n",
    "        \n",
    "    \n",
    "    def create_data(self, create_men=True):\n",
    "        '''\n",
    "        pass through manual sieves, collect mention pairs, and create final variable list for all \n",
    "        training and test data sets.\n",
    "        '''\n",
    "        cwd = os.getcwd()\n",
    "        if cwd == '/home/john/diploma':\n",
    "            pass\n",
    "        else:\n",
    "            os.chdir('/home/john/diploma')\n",
    "        check = True #for printing results if True\n",
    "        for i, nrow in self.newdf.iterrows():\n",
    "            docname = nrow.docname\n",
    "            print(docname)\n",
    "            docid = nrow.docid\n",
    "            mens = pd.read_csv('coref/mentions/{0}'.format(nrow.docname), delimiter='\\t')\n",
    "            deps = pd.read_csv('coref/df_ready/{0}'.format(nrow.docname), delimiter='\\t')\n",
    "#             mens = pd.read_csv('coref/mentions/73_ilf_schastlivy_otec.txt', delimiter='\\t') #for doc testing\n",
    "#             deps = pd.read_csv('coref/df_ready/73_ilf_schastlivy_otec.txt', delimiter='\\t')\n",
    "            mens['clust_head'] = '_' # create clust_head column.\n",
    "            deps['clust_head'] = '_'\n",
    "            self.manual_sieves(mens, deps, check, docname)        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102_beliajev_nad_bezdnoj.txt\n",
      "107_dragunsky_volshebnaja_sila_iskusstva.txt\n",
      "15_paustovsky_zhilcy_starogo_doma.txt\n",
      "2_astafiev_zhizn_prozhit.txt\n",
      "30_dojl_sluchaj.txt\n",
      "34_kassil_solnce_svetit.txt\n",
      "43_musatov_stozhary.txt\n",
      "44_nagibin_siren.txt\n",
      "53_beliajev_dom_s_prividenijami.txt\n",
      "5_petrushevskaya_v_detstve.txt\n",
      "67_zamiatin_kolumb.txt\n",
      "73_ilf_schastlivy_otec.txt\n",
      "andersen_motylek.txt\n",
      "bazhov_travyanaja_zapadenka.txt\n",
      "bunin_skazka.txt\n",
      "dostojevskij_podrostok.txt\n",
      "dovlatov_kompromiss_6.txt\n",
      "fet_knyaginya.txt\n",
      "gilyarovskij_moi_skitanija.txt\n",
      "gogol_zapiski_3.txt\n",
      "harms_upadanije.txt\n",
      "korolenko_mgnovenije.txt\n",
      "turgenev_veshnije_vody.txt\n",
      "2013_04_11_dotless_.txt\n",
      "2013_07_31_krebs_.txt\n",
      "lenta.ru-news-2014-01-19-cutshort.txt\n",
      "lenta.ru-news-2014-01-24-if.txt\n",
      "lenta.ru-news-2014-01-30-crimea.txt\n",
      "lenta.ru-news-2014-02-03-capitanic.txt\n",
      "lenta.ru-news-2014-02-03-london.txt\n",
      "lenta.ru-news-2014-02-03-name1.txt\n",
      "lenta.ru-news-2014-02-03-rucksack.txt\n",
      "lenta.ru-news-2014-02-04-party.txt\n",
      "lenta.ru-news-2014-02-04-pyramid.txt\n",
      "448-done.txt\n",
      "516-done.txt\n",
      "540-done.txt\n",
      "554-done.txt\n",
      "559-done.txt\n",
      "675-done.txt\n",
      "682-done.txt\n",
      "689-done.txt\n",
      "789-done.txt\n",
      "833-done.txt\n",
      "842-done.txt\n",
      "850-done.txt\n",
      "870-done.txt\n",
      "890-done.txt\n",
      "895-done.txt\n",
      "908-done.txt\n",
      "921-done.txt\n",
      "992-done.txt\n",
      "09Jan2014_bloomberg_sleep.html.txt\n",
      "strugackije_ponedelnik.txt\n",
      "2009-abbas5_ru.txt\n",
      "2009-abbas6_ru.txt\n",
      "2009-abbas7_ru.txt\n",
      "2009-abusada7_ru.txt\n",
      "2009-ahtisaari3_ru.txt\n",
      "2009-annan2_ru.txt\n",
      "2009-annan3_ru.txt\n",
      "2009-aslund24_ru.txt\n",
      "2009-asteiner3_ru.txt\n",
      "2009-asteiner5_ru.txt\n",
      "2009-avineri35_ru.txt\n",
      "2009-bakker3_ru.txt\n",
      "2009-bakker4_ru.txt\n",
      "2009-baradei1_ru.txt\n",
      "2009-barbarosie1_ru.txt\n",
      "2009-barnett1_ru.txt\n",
      "2009-barroso1_ru.txt\n",
      "2009-barroso3_ru.txt\n",
      "2009-beasley1_ru.txt\n",
      "2009-bebchuk1_ru.txt\n",
      "347-done.txt\n",
      "388-done.txt\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r59401.txt\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r67576.txt\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r68708.txt\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r69802.txt\n",
      "www.turpravda.ru-gr-halkidiki-Potidea_Palace-h13681-r72202.txt\n",
      "PhotoDescr1.txt\n",
      "PhotoDescr11.txt\n",
      "PhotoDescr15.txt\n",
      "nauka i zhizn_mars.txt\n",
      "nauka i zhizn_pererabotka.txt\n",
      "philology.ru-linguistics1-alpatov-12-out2.txt\n",
      "philology.ru-linguistics1-barannikov-46-out2.txt\n",
      "2.txt\n",
      "3.txt\n",
      "4.txt\n",
      "5.txt\n",
      "7.txt\n",
      "9.txt\n",
      "10.txt\n",
      "11.txt\n",
      "12.txt\n",
      "15.txt\n",
      "19.txt\n",
      "51.txt\n",
      "52.txt\n",
      "53.txt\n",
      "54.txt\n",
      "55.txt\n",
      "57.txt\n",
      "64.txt\n",
      "65.txt\n",
      "66.txt\n",
      "67.txt\n",
      "68.txt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-162-4e4356a2f40f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# example1.fix_deps()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# example1.create_mentions()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mexample1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-05865a9adbd9>\u001b[0m in \u001b[0;36mcreate_data\u001b[0;34m(self, create_men)\u001b[0m\n\u001b[1;32m   1013\u001b[0m             \u001b[0mmens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clust_head'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;31m# create clust_head column.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m             \u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'clust_head'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_sieves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-05865a9adbd9>\u001b[0m in \u001b[0;36mmanual_sieves\u001b[0;34m(self, mens, deps, check, docname)\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0mmentions_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'clust_head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'part_men'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m         \u001b[0mmentions_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_match_sieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 601\u001b[0;31m         \u001b[0mdeps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_match_sieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscourse_sieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#change to mentions_final in final version!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmentions_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrel_sieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-05865a9adbd9>\u001b[0m in \u001b[0;36mhead_match_sieve\u001b[0;34m(self, mens, mentions_final, deps, manual, gold)\u001b[0m\n\u001b[1;32m    642\u001b[0m                     \u001b[0mmen_lem2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmention2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mi4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m                         \u001b[0;32mif\u001b[0m \u001b[0mmen2_sh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcheck2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tk_shifts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'series'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'True'\u001b[0m                        \u001b[0;32mand\u001b[0m \u001b[0mskip2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m                             \u001b[0mmention2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                             \u001b[0mskip2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john/diploma/env/lib/python3.5/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john/diploma/env/lib/python3.5/site-packages/pandas/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   2167\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2168\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 2169\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   2170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minferred_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'integer'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'boolean'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inpt = '../../coref/rucoref_texts/fiction/5_petrushevskaya_v_detstve.txt'\n",
    "out = '../../output.text'\n",
    "path1 = 'syntaxnet/models/parsey_universal/parse.sh'    #path to parsh.sh script\n",
    "path2 = 'syntaxnet/models/parsey_universal/Russian-SynTagRus'    #path to your dependencies model\n",
    "\n",
    "modp = 'udpipe/src/russian-syntagrus-ud-2.0-conll17-170315.udpipe'\n",
    "outp = 'coref/new_parsed_texts'\n",
    "textp = 'coref/rucoref_texts'\n",
    "\n",
    "example1 = Corefsetup()\n",
    "newdf = example1.prepdf()\n",
    "# example1.create_dep(textp, modp, outp)\n",
    "# example1.make_ner_dict(textp)\n",
    "# example1.fix_deps()\n",
    "# example1.create_mentions()\n",
    "example1.create_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Coreftrain:\n",
    "    \n",
    "    def __init__(self, newdf):\n",
    "        self.train_noun = pd.DataFrame(columns=['docid', 'start_ref', 'cor', 'incor'])\n",
    "        self.gold_noun =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.no_gold_noun =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.train_ne = pd.DataFrame(columns=['docid', 'start_ref', 'cor', 'incor'])\n",
    "        self.gold_ne =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.no_gold_ne =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.train_pron = pd.DataFrame(columns=['docid', 'start_ref', 'cor', 'incor'])\n",
    "        self.gold_pron =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.no_gold_pron =  pd.DataFrame(columns=['docid', 'start_ref', 'refs'])\n",
    "        self.answers_noun = pd.DataFrame() # hold answers for training sets. separtatedby type. 9 total.\n",
    "        self.answers_ne = pd.DataFrame()\n",
    "        self.answers_pron = pd.DataFrame()\n",
    "        self.train_var_noun = pd.DataFrame()#training data\n",
    "        self.train_var_ne = pd.DataFrame()\n",
    "        self.train_var_pron = pd.DataFrame()\n",
    "        self.gold_var_noun = pd.DataFrame()#start vars for nouns\n",
    "        self.no_gold_var_noun = pd.DataFrame()\n",
    "        self.gold_var_ne = pd.DataFrame() #start vars for NEs\n",
    "        self.no_gold_var_ne = pd.DataFrame()\n",
    "        self.gold_var_pron = pd.DataFrame()#start vars for pronouns\n",
    "        self.no_gold_var_pron = pd.DataFrame()\n",
    "        self.idstr_noun = pd.DataFrame() # holds ids for merge reference.\n",
    "        self.idstr_ne = pd.DataFrame()\n",
    "        self.idstr_pron = pd.DataFrame()\n",
    "        self.ids_ne_gold = pd.DataFrame()# ne ids\n",
    "        self.ids_ne_nogold = pd.DataFrame()\n",
    "        self.ids_noun_gold = pd.DataFrame()# ne noun\n",
    "        self.ids_noun_nogold = pd.DataFrame()\n",
    "        self.ids_pron_gold = pd.DataFrame()# ne pron\n",
    "        self.ids_pron_nogold = pd.DataFrame()\n",
    "        self.newdf = newdf\n",
    "        \n",
    "    \n",
    "        #merge functions from previous doc. Was decided that corefsetup already to large, hence the repetition.\n",
    "    \n",
    "    def old_morph(self, headmen, head_pos):\n",
    "        '''\n",
    "        gathers morphological info from head mention.\n",
    "        '''\n",
    "        leng = len(headmen['morph'].values[0].split('|'))\n",
    "        if leng == 4:\n",
    "            ment_anim = headmen['morph'].values[0].split('|')[0].replace('Animacy=', '').split(',')\n",
    "            ment_case = headmen['morph'].values[0].split('|')[1].replace('Case=', '') #only one possible value.\n",
    "            ment_gen = headmen['morph'].values[0].split('|')[2].replace('Gender=', '').split(',')\n",
    "            ment_num = headmen['morph'].values[0].split('|')[3].replace('Number=', '').split(',')\n",
    "        elif leng == 3:\n",
    "            ment_anim = None\n",
    "            ment_case = headmen['morph'].values[0].split('|')[0].replace('Case=', '') #only one possible value.\n",
    "            ment_gen = headmen['morph'].values[0].split('|')[1].replace('Gender=', '').split(',')\n",
    "            ment_num = headmen['morph'].values[0].split('|')[2].replace('Number=', '').split(',')\n",
    "        else:\n",
    "            ment_anim = None\n",
    "            ment_case = None\n",
    "            ment_gen = None\n",
    "            ment_num = None\n",
    "        return ment_anim, ment_case, ment_gen, ment_num\n",
    "    \n",
    "    \n",
    "    def new_morph(self, new_anim, new_gen, new_num, ment_anim, ment_gen, ment_num, mer_men):\n",
    "        '''\n",
    "        combines morphological features of mentions with head mention.\n",
    "        '''\n",
    "        if not ment_anim == None:\n",
    "            if len(mer_men['morph'].values[0].split('|')) == 4:\n",
    "                mer_anim = mer_men['morph'].values[0].split('|')[0].replace('Animacy=', '').split(', ')\n",
    "                mer_gen = mer_men['morph'].values[0].split('|')[2].replace('Gender=', '').split(', ')\n",
    "                mer_num = mer_men['morph'].values[0].split('|')[3].replace('Number=', '').split(', ')\n",
    "                for mer_a in mer_anim:\n",
    "                    if mer_a not in ment_anim: #merging morphological features, if different.\n",
    "                        new_anim.append(mer_a)\n",
    "                for mer_g in mer_gen:\n",
    "                    if mer_g not in ment_gen:\n",
    "                        new_gen.append(mer_g)\n",
    "                for mer_n in mer_num:\n",
    "                    if mer_n not in ment_num:\n",
    "                        new_num.append(mer_n)\n",
    "                no_merge = False\n",
    "            else:\n",
    "                no_merge = True\n",
    "        elif not ment_gen == None:\n",
    "            mer_gen = mer_men['morph'].values[0].split('|')[1].replace('Gender=', '').split(', ')\n",
    "            mer_num = mer_men['morph'].values[0].split('|')[2].replace('Number=', '').split(', ')\n",
    "            new_anim = None\n",
    "            for mer_g in mer_gen:\n",
    "                if mer_g not in ment_gen:\n",
    "                    new_gen.append(mer_g)\n",
    "            for mer_n in mer_num:\n",
    "                if mer_n not in ment_num:\n",
    "                    new_num.append(mer_n)\n",
    "            no_merge = False\n",
    "        else:\n",
    "            new_anim = None\n",
    "            new_gen = None\n",
    "            new_num = None\n",
    "            no_merge = True\n",
    "        return new_anim, new_gen, new_num, no_merge\n",
    "          \n",
    "\n",
    "# merge types\n",
    "        \n",
    "   \n",
    "    def morph_merge(self, headmen, merges, mens):\n",
    "        '''\n",
    "        merges morphology of mentions to head mention.\n",
    "        '''\n",
    "        head_pos = headmen['udpos']\n",
    "        ment_anim, ment_case, ment_gen, ment_num = self.old_morph(headmen, head_pos) #creates list of all morph features of head mention.\n",
    "        new_anim = []\n",
    "        new_gen = []\n",
    "        new_num = []\n",
    "        if not ment_anim == None:\n",
    "            new_anim.extend(ment_anim)\n",
    "        elif not ment_case == None:\n",
    "            new_gen.extend(ment_gen)\n",
    "            new_num.extend(ment_num)\n",
    "        for mer in merges:\n",
    "            mer_men = mens[mens['shift'] == mer]\n",
    "            new_anim, new_gen, new_num, no_merge = self.new_morph(new_anim, new_gen, new_num, ment_anim, ment_gen, ment_num, mer_men)\n",
    "        if not new_anim == None and no_merge == False:\n",
    "            new_anim = \", \".join(list(set(new_anim))) #remove repeating features.\n",
    "            new_gen = \", \".join(list(set(new_gen)))\n",
    "            new_num = \", \".join(list(set(new_num)))\n",
    "            new_mor = 'Animacy={0}|Case={1}|Gender={2}|Number={3}'.format(new_anim, ment_case, new_gen, new_num)\n",
    "        elif no_merge == False and not new_gen == None:\n",
    "            new_gen = \", \".join(list(set(new_gen)))\n",
    "            new_num = \", \".join(list(set(new_num)))\n",
    "            new_mor = 'Case={0}|Gender={1}|Number={2}'.format(ment_case, new_gen, new_num)\n",
    "        else:\n",
    "            new_mor = headmen['morph'].values[0]\n",
    "        return new_mor   \n",
    "    \n",
    "    \n",
    "    def sid_merge(self, headmen, headmen_id, merges, mens):\n",
    "        '''\n",
    "        merges sentence id info, for later calculation of distances, as program chooses closest distance\n",
    "        when considering sentence range. Creates clust_head category for referencing back to head in order to\n",
    "        gather cluster level info.\n",
    "        '''\n",
    "        head_sid = headmen['sid'].values #extract all values\n",
    "        new_sids = []\n",
    "        new_sids.extend(head_sid)\n",
    "        clust_head_of_head = mens.ix[headmen_id, 'clust_head'] # find head of head mention.\n",
    "        for mer in merges:\n",
    "            mer_men = mens[mens['shift'] == mer]\n",
    "            mer_sid = mer_men['sid'].values\n",
    "            mer_id = mer_men.index.tolist()[0] #check functioning!\n",
    "            new_sids.extend(mer_sid)\n",
    "            if not clust_head_of_head == '_': # adds head_clust of headmen to mention if it exists.\n",
    "                mens.ix[mer_id, 'clust_head'] = clust_head_of_head\n",
    "            else:\n",
    "                mens.ix[mer_id, 'clust_head'] = headmen['shift'].values[0] #shift head\n",
    "        new_sids = ', '.join(list(set([str(x) for x in new_sids])))\n",
    "        mens.ix[headmen_id, 'sid_corefs'] = new_sids\n",
    "        return mens\n",
    "            \n",
    "            \n",
    "            \n",
    "    def merge(self, mens, men_dict, manual, morph_merge, gold): # main merge function.\n",
    "        '''\n",
    "        merges data for corefering mentions(i.e. sid, shift of all), appending necessary \n",
    "        info to head mention (often most informative mention.), removes all child \n",
    "        mentions from mention list, replaces child mention pairings in candidate list with head word. \n",
    "        Because features are aggregated across all mentions in cluster, will not make impact on performance.\n",
    "        '''      \n",
    "        if manual == True:\n",
    "            for ment, merges in men_dict.items():\n",
    "                skip = False\n",
    "                headmen = mens[mens['shift'] == ment]\n",
    "                if gold == True: #to select for merging in gold model, only those that exist.\n",
    "                    indexes = []\n",
    "                    if not headmen.empty:\n",
    "                        indexes.append(ment)\n",
    "                    for mer in merges:\n",
    "                        mermen = mens[mens['shift'] == mer]\n",
    "                        if not mermen.empty:\n",
    "                            indexes.append(mer)\n",
    "                    if len(indexes) > 0:\n",
    "                        indexes.sort() # to get the most early occuring mention first in order.\n",
    "                        head_sh = indexes[0]\n",
    "                        headmen = mens[mens['shift'] == head_sh]\n",
    "                        headmen_id = headmen.index.tolist()[0]\n",
    "                        del indexes[0]\n",
    "                        merges = indexes \n",
    "                    else:\n",
    "                        skip = True\n",
    "                else:\n",
    "                    try: #because of trying to train mens and deps at same time\n",
    "                        headmen_id = headmen.index.tolist()[0]\n",
    "                    except IndexError:\n",
    "                        print('first ment error!', ment)               \n",
    "                if skip == False:\n",
    "                    mens.ix[headmen_id, 'corefs'] = ', '.join([str(x) for x in merges]) # add shifts of all corefering mentions\n",
    "                    if len(merges) > 0:\n",
    "                        if morph_merge == True:\n",
    "                            new_mor = self.morph_merge(headmen, merges, mens)\n",
    "                            mens.ix[headmen_id, 'morph'] = new_mor\n",
    "                        mens = self.sid_merge(headmen, headmen_id, merges, mens) \n",
    "                    mens = mens.fillna('_')\n",
    "            return mens  \n",
    "    \n",
    "    \n",
    "    def form_frame(self, deps, mens, type_all, l_sids, r_sids, sid, typ, goldm):\n",
    "        if type_all == 'pron':\n",
    "            if typ in ('pron', 'poss'):\n",
    "                rng = 3\n",
    "                frame = self.create_frames(l_sids, r_sids, sid, rng, deps, goldm, mens)#sentence frames for candidate searching.\n",
    "            elif typ in ('refl', 'rel'):\n",
    "                if goldm == False:\n",
    "                    frame = deps[deps['sid'] == sid]\n",
    "                else:\n",
    "                    frame = mens[mens['sid'] == sid]\n",
    "            else:\n",
    "                frame = []\n",
    "        elif type_all == 'ne':\n",
    "            if typ == 'NE':\n",
    "                rng = 5\n",
    "                frame = self.create_frames(l_sids, r_sids, sid, rng, deps, goldm, mens)\n",
    "            else:\n",
    "                frame = []\n",
    "        elif type_all == 'noun':\n",
    "            if typ == 'noun':\n",
    "                rng = 4\n",
    "                frame = self.create_frames(l_sids, r_sids, sid, rng, deps, goldm, mens)\n",
    "            else:\n",
    "                frame = [] #catch gold mens to deps descrepancies for training set. Just skip.\n",
    "        return frame\n",
    "    \n",
    "            \n",
    "    def set_up_frames(self, ment, typ, type_all, deps, mens, train, goldm):  \n",
    "        '''\n",
    "        sort frame ranges and frames for different mentions.\n",
    "        '''\n",
    "        sid = int(ment['sid'])\n",
    "        sid_max = int(deps['sid'][deps['sid'].idxmax()])\n",
    "        ment_sh = ment['shift']\n",
    "        l_sids = 0\n",
    "        r_sids = 0\n",
    "        if sid < 4: #finds left and right boundaries if sentence at beginning or end of doc.\n",
    "            l_sids = [x for x in range(1, sid+1)]\n",
    "        elif (sid_max - sid) < 4:\n",
    "            r_sids = [x for x in range(sid, sid_max+1)]\n",
    "        frames = self.form_frame(deps, mens, type_all, l_sids, r_sids, sid, typ, goldm)\n",
    "        return frames\n",
    "    \n",
    "\n",
    "    def create_frames(self, l_sids, r_sids, sid, rng, deps, goldm, mens):\n",
    "        '''\n",
    "        creates frames for mention search.\n",
    "        '''\n",
    "        if goldm == False:\n",
    "            if not l_sids == 0 and not r_sids == 0:\n",
    "                l_len = len(l_sids)\n",
    "                r_len = len(r_sids)\n",
    "                if l_len > 0 and r_len > 0:\n",
    "                    frame = deps.query('{0} <= sid <= {1}'.format(l_sids, r_sids))\n",
    "                elif l_len > 0 and not r_len > 0:\n",
    "                    frame = deps.query('{0} <= sid <= {1}'.format(l_sids, sid+rng))\n",
    "                elif r_len > 0 and not l_len > 0:\n",
    "                    frame = deps.query('{0} <= sid <= {1}'.format(sid-rng, r_sids))\n",
    "            else:\n",
    "                frame = deps.query('{0} <= sid <= {1}'.format(sid-rng, sid+rng))\n",
    "        else:\n",
    "            if not l_sids == 0 and not r_sids == 0:\n",
    "                l_len = len(l_sids)\n",
    "                r_len = len(r_sids)\n",
    "                if l_len > 0 and r_len > 0:\n",
    "                    frame = mens.query('{0} <= sid <= {1}'.format(l_sids, r_sids))\n",
    "                elif l_len > 0 and not r_len > 0:\n",
    "                    frame = mens.query('{0} <= sid <= {1}'.format(l_sids, sid+rng))\n",
    "                elif r_len > 0 and not l_len > 0:\n",
    "                    frame = mens.query('{0} <= sid <= {1}'.format(sid-rng, r_sids))\n",
    "            else:\n",
    "                frame = mens.query('{0} <= sid <= {1}'.format(sid-rng, sid+rng))\n",
    "        return frame\n",
    "\n",
    "    \n",
    "    def create_test_sets(self, ment, deps, frame, docid, df, ment_ser, goldm):\n",
    "        '''\n",
    "        appends rows to test sets for each ment\n",
    "        '''\n",
    "        ments = []\n",
    "        ment_sh = ment['shift']\n",
    "        ment_shs = ment['tk_shifts'].split(', ')\n",
    "        if not len(frame) == 0: #aviod skipped mentions.\n",
    "            for i, m in frame.iterrows():\n",
    "                m_sh = m['shift']\n",
    "                if m_sh in ment_shs or m_sh == ment_sh: # so not to capture mentions in mentions as possibly coreferent if they overlap at all.\n",
    "                    pass\n",
    "                elif not m['full_men'] == '_':\n",
    "                    ments.append(m_sh)\n",
    "            ments = list(set(ments))\n",
    "            tempval = {'docid':[docid], 'start_ref':[ment_sh], 'refs':[ments], 'series':[ment_ser]}\n",
    "            tempdf = pd.DataFrame(tempval)\n",
    "            df = df.append(tempdf)\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def create_candidates(self, mens, deps, train, gold, no_gold, docid, \\\n",
    "                          train_ids, test_ids, type_all):\n",
    "        '''\n",
    "        creates initial candidate lists for training, gold mentions, and no gold mentions.\n",
    "        '''\n",
    "        if docid in test_ids:\n",
    "            for i, ment in deps.iterrows():\n",
    "                if not ment['full_men'] == '_' and not ment['udpos'] == 'PUNCT': #avoid non mentions and parsing errors.\n",
    "                    ment_typ = ment['type']\n",
    "                    if 'series' not in ment:\n",
    "                        ment['series'] = '_'\n",
    "                    ment_ser = ment['series']\n",
    "                    frame = self.set_up_frames(ment, ment_typ, type_all, deps, mens, train=False, goldm=False)\n",
    "                    no_gold = self.create_test_sets(ment, deps, frame, docid, no_gold, ment_ser, goldm=False)\n",
    "        for i, ment in mens.iterrows():\n",
    "            if not ment['udpos'] == 'PUNCT' and not ment['full_men'] =='_':\n",
    "                if 'series' not in ment:\n",
    "                    ment['series'] = '_'\n",
    "                if docid in test_ids:\n",
    "                    ment_typ = ment['type']\n",
    "                    ment_ser = ment['series']\n",
    "                    frame = self.set_up_frames(ment, ment_typ, type_all, deps, mens, train=False, goldm=True)\n",
    "                    gold = self.create_test_sets(ment, deps, frame, docid, gold, ment_ser, goldm=True)\n",
    "                else:\n",
    "                    sid = int(ment['sid'])\n",
    "                    ment_sh = ment['shift']\n",
    "                    ment_typ = ment['type']\n",
    "                    ment_ser = ment['series']\n",
    "                    frame = self.set_up_frames(ment, ment_typ, type_all, deps, mens, train=True, goldm=False) # create frame.\n",
    "                    if len(frame) == 0:\n",
    "                        pass\n",
    "                    else:\n",
    "                        cor_id = ment['chain_id']\n",
    "                        cor_men = mens['shift'][mens['chain_id'] == cor_id].tolist()\n",
    "                        cor_ments = []\n",
    "                        for cor in cor_men:\n",
    "                            ment_temp = frame[frame['shift'] == cor]\n",
    "                            cor_ments.append(ment_temp)\n",
    "                        n_cor_ments = []\n",
    "                        for df in cor_ments:\n",
    "                            if df.empty:\n",
    "                                pass\n",
    "                            elif df['shift'].values[0] == ment_sh:\n",
    "                                pass\n",
    "                            else:\n",
    "                                n_cor_ments.append(df)\n",
    "                        if len(n_cor_ments) == 0: #for refs that can't be found within the given frame.\n",
    "                            pass\n",
    "                        else:\n",
    "                            cor = random.choice(n_cor_ments) # in order to randomly choose mentions.\n",
    "                            cor_sh = cor['shift'].values[0]\n",
    "                            if cor_sh > ment_sh: #select incorrect only if it's between correct\n",
    "                                incors = frame[(ment_sh < frame['shift']) & (frame['shift'] < cor_sh)]\n",
    "                            elif ment_sh > cor_sh:\n",
    "                                incors = frame[(cor_sh < frame['shift']) & (frame['shift'] < ment_sh)]\n",
    "                            incor_ments = []\n",
    "                            for i, incorr in incors.iterrows():\n",
    "                                if incorr['shift'] in cor_men or incorr['shift'] == ment_sh:\n",
    "                                    pass\n",
    "                                elif not incorr['full_men'] == '_': #weed out non mentions\n",
    "                                    incor_ments.append(incorr)\n",
    "                            if len(incor_ments) == 0: # if no possible mentions in between.\n",
    "                                if cor_sh > ment_sh: #select any possible mention in frame.\n",
    "                                    incors = frame[(cor_sh < frame['shift']) | (frame['shift'] < ment_sh)]\n",
    "                                elif ment_sh > cor_sh:\n",
    "                                    incors = frame[(ment_sh < frame['shift']) | (frame['shift'] < cor_sh)]\n",
    "                                for i, incorr in incors.iterrows():\n",
    "                                    if incorr['shift'] in cor_men or incorr['shift'] == ment_sh:\n",
    "                                        pass\n",
    "                                    elif not incorr['full_men'] == '_': #weed out non mentions\n",
    "                                        incor_ments.append(incorr)\n",
    "                            if len(incor_ments) == 0: # If no other posible mentions in frame besides correct men\n",
    "                                incors = deps[deps['sid'] == sid + 1] # add following sent so there is a value\n",
    "                                for i, incorr in incors.iterrows():\n",
    "                                    if incorr['shift'] in cor_men or incorr['shift'] == ment_sh:\n",
    "                                        pass\n",
    "                                    elif not incorr['full_men'] == '_': #weed out non mentions\n",
    "                                        incor_ments.append(incorr)\n",
    "                                if len(incor_ments) == 0: # If no other posible mentions in frame besides correct men\n",
    "                                    incors = deps[deps['sid'] == sid - 1] # add following sent so there is a value\n",
    "                                    for i, incorr in incors.iterrows():\n",
    "                                        if incorr['shift'] in cor_men or incorr['shift'] == ment_sh:\n",
    "                                            pass\n",
    "                                        elif not incorr['full_men'] == '_': #weed out non mentions\n",
    "                                            incor_ments.append(incorr)\n",
    "                            incor = random.choice(incor_ments)\n",
    "                            incor_sh = incor['shift']\n",
    "                            tempval = {'docid':[docid], 'start_ref':[ment_sh], 'cor':[cor_sh], 'incor':[incor_sh], 'series':[ment_ser]}\n",
    "                            tempdf = pd.DataFrame(tempval)\n",
    "                            train = train.append(tempdf)\n",
    "        return train, gold, no_gold\n",
    "      \n",
    "    \n",
    "    \n",
    "    def compare(self, title, val, temp_ref, start_ment, ref_ment):\n",
    "        '''\n",
    "        module for comparing values of two mentions. Appends 1 for True, 0 for False to given title.\n",
    "        '''\n",
    "        if val == 'morph': #because need to split morph values.\n",
    "            start_morph = start_ment[val].values[0].split('|')\n",
    "            try:\n",
    "                ref_morph = ref_ment[val].values[0].split('|')\n",
    "            except IndexError:\n",
    "                print(ref_ment)\n",
    "                print(title)\n",
    "                sys.exit()\n",
    "            stopg = False # set stops to false default\n",
    "            stopa = False\n",
    "            stopn = False\n",
    "            stopp = False\n",
    "            stopgr = False\n",
    "            stopar = False\n",
    "            stopnr = False\n",
    "            stoppr = False\n",
    "            for morph in start_morph: # determine existance of morph\n",
    "                if title == 'genderm':\n",
    "                    if morph.startswith('Gender=') and stopg == False:\n",
    "                        stopg = True\n",
    "                elif title == 'animacym':\n",
    "                    if morph.startswith('Animacy=') and stopa == False:\n",
    "                        stopa = True\n",
    "                elif title == 'numberm':\n",
    "                    if morph.startswith('Number=') and stopn == False:\n",
    "                        stopn = True\n",
    "                elif title == 'person_match':\n",
    "                    if morph.startswith('Person=') and stopp == False:\n",
    "                        stopp = True\n",
    "                        \n",
    "            for morph in ref_morph:\n",
    "                if title == 'genderm':\n",
    "                    if morph.startswith('Gender=') and stopgr == False:\n",
    "                        stopgr = True\n",
    "                elif title == 'animacym':\n",
    "                    if morph.startswith('Animacy=') and stopar == False:\n",
    "                        stopar = True\n",
    "                elif title == 'numberm':\n",
    "                    if morph.startswith('Number=') and stopnr == False:\n",
    "                        stopnr = True\n",
    "                elif title == 'person_match':\n",
    "                    if morph.startswith('Person=') and stoppr == False:\n",
    "                        stoppr= True\n",
    "                        \n",
    "                        \n",
    "            if val == 'clustagree':   \n",
    "                if stopg == False:\n",
    "                    start_val_gen = None\n",
    "                if stopa == False:\n",
    "                    start_val_ani = None\n",
    "                if stopn == False:\n",
    "                    start_val_num = None\n",
    "                if stopgr == False:\n",
    "                    ref_val_gen = None\n",
    "                if stopar == False:\n",
    "                    ref_val_ani = None\n",
    "                if stopnr == False:\n",
    "                    ref_val_num = None\n",
    "                    \n",
    "                for morph in start_morph:\n",
    "                    if morph.startswith('Gender='):\n",
    "                        start_val_gen = morph\n",
    "                    if morph.startswith('Animacy='):\n",
    "                        start_val_ani = morph\n",
    "                    if morph.startswith('Number='):\n",
    "                        start_val_num = morph\n",
    "                        \n",
    "                    \n",
    "                for morph in ref_morph:\n",
    "                    if morph.startswith('Gender='):\n",
    "                        ref_val_gen = morph\n",
    "                    if morph.startswith('Animacy='):\n",
    "                        ref_val_ani = morph\n",
    "                    if morph.startswith('Number='):\n",
    "                        ref_val_num = morph\n",
    "                if start_val_gen == ref_val_gen and start_val_ani == ref_val_ani and start_val_num == ref_val_num \\\n",
    "                and not (start_val_gen == None or start_val_ani == None or start_val_num == None or ref_val_gen == None \\\n",
    "                or ref_val_ani == None or ref_val_num == None):\n",
    "                    temp_ref[title] = [1]\n",
    "                else:\n",
    "                    temp_ref[title] = [0]\n",
    "                return temp_ref\n",
    "\n",
    "                \n",
    "            else:\n",
    "                if (stopg or stopa or stopa or stopp) == False:\n",
    "                    start_val = None\n",
    "                \n",
    "                if (stopgr or stopar or stopar or stoppr) == False:\n",
    "                    ref_val = None\n",
    "\n",
    "                    \n",
    "                for morph in start_morph:\n",
    "                    if title == 'genderm':\n",
    "                        if morph.startswith('Gender='):\n",
    "                            start_val = morph\n",
    "                    elif title == 'animacym':\n",
    "                        if morph.startswith('Animacy='):\n",
    "                            start_val = morph\n",
    "                    elif title == 'numberm':\n",
    "                        if morph.startswith('Number='):\n",
    "                            start_val = morph\n",
    "                    elif title == 'person_match':\n",
    "                        if morph.startswith('Person='):\n",
    "                            start_val = morph\n",
    "                            \n",
    "                    \n",
    "                for morph in ref_morph:\n",
    "                    if title == 'genderm':\n",
    "                        if morph.startswith('Gender='):\n",
    "                            ref_val = morph\n",
    "                    elif title == 'animacym':\n",
    "                        if morph.startswith('Animacy='):\n",
    "                            ref_val = morph\n",
    "                    elif title == 'numberm':\n",
    "                        if morph.startswith('Number='):\n",
    "                            ref_val = morph\n",
    "                    elif title == 'person_match':\n",
    "                        if morph.startswith('Person='):\n",
    "                            ref_val = morph\n",
    "            \n",
    "        else: # for all other compares, without special format\n",
    "        \n",
    "            start_val = start_ment[val].values[0]\n",
    "            ref_val = ref_ment[val].values[0]\n",
    "#         if ('start_val' in locals() or 'start_val' in globals()) \\\n",
    "#         and ('ref_val' in locals() or 'ref_val' in globals()):\n",
    "        \n",
    "        if not title == 'clustagree':\n",
    "            try:\n",
    "                if ref_val == start_val and not (ref_val or start_val) == None:\n",
    "                    temp_ref[title] = [1]\n",
    "                else:\n",
    "                    temp_ref[title] = [0]\n",
    "            except UnboundLocalError:\n",
    "                print(start_morph)\n",
    "                print(ref_morph)\n",
    "                print(title)\n",
    "                print(val)\n",
    "                sys.exit()\n",
    "\n",
    "#         else:\n",
    "#             temp_ref[title] = [0]\n",
    "        \n",
    "        return temp_ref\n",
    "            \n",
    "            \n",
    "    def partialheadm(self, title, val, temp_ref, start_ment, ref_ment, deps):\n",
    "        '''\n",
    "        relaxed head mention match. If any nouns or proper nouns in mention match returns 1 for true.\n",
    "        '''\n",
    "        start_shs = start_ment[val].values[0].split(',')\n",
    "        ref_shs = ref_ment[val].values[0].split(',')\n",
    "        if len(ref_shs) > 0:\n",
    "            for stsh in start_shs:\n",
    "                st = deps[deps['shift'] == float(stsh)]\n",
    "                st_pos = st['udpos'].values[0]\n",
    "                if st_pos in ('NOUN', 'PROPN'): #so that not just pairing anything\n",
    "                    st_lem = st['lemma'].values[0]\n",
    "                    for refsh in ref_shs:\n",
    "                        try:\n",
    "                            rf = deps[deps['shift'] == float(refsh)]\n",
    "                        except ValueError:\n",
    "                            print(ref_shs)\n",
    "                            print(ref_ment)\n",
    "                            print(title)\n",
    "                            sys.exit()\n",
    "                        rf_lem = rf['lemma'].values[0]\n",
    "                        if st_lem == rf_lem:  # compare the two\n",
    "                            temp_ref[title] = [1]\n",
    "            if temp_ref[title].empty:\n",
    "                temp_ref[title] = [0]\n",
    "        else:\n",
    "            temp_ref[title] = [0]\n",
    "        return temp_ref\n",
    "                    \n",
    "        \n",
    "    def distance(self, title, val, temp_ref, start_ment, ref_ment):\n",
    "        '''\n",
    "        finds distance bewteen given start and finish, depending on variable.\n",
    "        '''\n",
    "        start_sid = start_ment[val].values[0]\n",
    "        ref_sid = ref_ment[val].values[0]\n",
    "        dif = int(start_sid) - int(ref_sid) #can be negative to account for direction.\n",
    "        temp_ref[title] = [dif]\n",
    "        return temp_ref\n",
    "           \n",
    "        \n",
    "    def sing_to_plur(self, morph):\n",
    "        '''\n",
    "        Change number of series to plural\n",
    "        '''\n",
    "        morph = ['Number=Plur' if x=='Number=' else x for x in morph]\n",
    "        morph = '|'.join(morph)\n",
    "        return morph\n",
    "      \n",
    "        \n",
    "    def find_val(self, title, val, temp_ref, start_ment, find):\n",
    "        '''\n",
    "        returns 1 for true if head begins with a determiner.\n",
    "        '''\n",
    "        if val == 'tk_shifts':\n",
    "            st_first = start_ment[val].values[0]\n",
    "            if not type(st_first) == float:\n",
    "                st_first = st_first.split(',')[0]\n",
    "            st_f_pos = start_ment['udpos'].values[0]\n",
    "            if st_f_pos == find:\n",
    "                temp_ref[title] = [1]\n",
    "            else:\n",
    "                temp_ref[title] = [0]\n",
    "        elif val in ('deprel', 'type', 'shift', 'containnum'):\n",
    "            st_first = start_ment[val].values[0]\n",
    "            if st_first == find:\n",
    "                temp_ref[title] = [1]\n",
    "            else:\n",
    "                temp_ref[title] = [0]\n",
    "        return temp_ref\n",
    "        \n",
    "        \n",
    "    def get_val(self, title, val, temp_ref, start_ment):\n",
    "        temp_ref[title] = [start_ment[val].values[0]]\n",
    "        return temp_ref\n",
    "    \n",
    "    \n",
    "    def comb_attr(self, title, val, temp_ref, start_ment, ref_ment):\n",
    "        start_val = start_ment[val].values[0]\n",
    "        ref_val = ref_ment[val].values[0]\n",
    "        new_val = '-'.join((start_val, ref_val))\n",
    "        temp_ref[title] = [new_val]\n",
    "        return temp_ref\n",
    "        \n",
    "    \n",
    "    def clust_info(self, title, val, temp_ref, start_ment, ref_ment, deps, act):\n",
    "        '''\n",
    "        for gathering and comparing cluster level info.\n",
    "        '''\n",
    "        ref_head = deps[deps['shift'] == float(ref_ment['head'].values[0])]\n",
    "        start_head = deps[deps['shift'] == float(start_ment['head'].values[0])]\n",
    "        if act == 'compare':\n",
    "            title = title\n",
    "            val = 'morph'\n",
    "            temp_ref.merge(self.compare(title, val, temp_ref, start_ment, ref_ment))\n",
    "        return temp_ref\n",
    "           \n",
    "        \n",
    "    def make_vars(self, deps, starts, refs, var_list, typ, train):\n",
    "        '''\n",
    "        makes vars sets for each mention.\n",
    "        '''\n",
    "        all_refs = []#list for temp storing of mention comparisons.\n",
    "        for start in starts:\n",
    "            for ref in refs:\n",
    "                ref_ment = deps[deps['shift'] == ref]\n",
    "                temp_ref = pd.DataFrame(columns=var_list)\n",
    "                if 'genderm' in var_list:\n",
    "                    title = 'genderm'\n",
    "                    val = 'morph'\n",
    "                    temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                if 'numberm' in var_list:\n",
    "                    title = 'numberm'\n",
    "                    val = 'morph'\n",
    "                    temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                if 'headm' in var_list:\n",
    "                    title = 'headm'\n",
    "                    val = 'lemma'\n",
    "                    temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                if 'partialheadm' in var_list:\n",
    "                    title = 'partialheadm'\n",
    "                    val = 'tk_shifts'\n",
    "                    temp_ref.merge(self.partialheadm(title, val, temp_ref, start, ref_ment, deps))\n",
    "                if 'sentdist' in var_list:\n",
    "                    title = 'sentdist'\n",
    "                    val = 'sid'\n",
    "                    temp_ref.merge(self.distance(title, val, temp_ref, start, ref_ment))\n",
    "                if 'detmen' in var_list:\n",
    "                    title = 'detmen'\n",
    "                    val = 'tk_shifts'\n",
    "                    find = 'DET'\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, start, find))\n",
    "                if 'detref' in var_list:\n",
    "                    title = 'detref'\n",
    "                    val = 'tk_shifts'\n",
    "                    find = 'DET'\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, ref_ment, find))\n",
    "                if 'nsubjant' in var_list:\n",
    "                    title = 'nsubjant'\n",
    "                    val = 'deprel'\n",
    "                    find = 'nsubj'\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, ref_ment, find))\n",
    "                if 'nsubjmen' in var_list:\n",
    "                    title = 'nsubjmen'\n",
    "                    val = 'deprel'\n",
    "                    find = 'nsubj'\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, start, find))\n",
    "                if 'nar_match' in var_list:\n",
    "                    if not start['nar'].values[0] == '_':\n",
    "                        title = 'nar_match'\n",
    "                        val = 'nar'\n",
    "                        temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                if 'antecedenttype' in var_list:\n",
    "                    title = 'antecedenttype'\n",
    "                    val = 'type'\n",
    "                    temp_ref.merge(self.get_val(title, val, temp_ref, ref_ment))\n",
    "                if 'namepartref' in var_list:\n",
    "                    title = 'namepartref'\n",
    "                    val = 'deprel'\n",
    "                    find = 'flat:name'\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, ref_ment, find))\n",
    "                if 'isrefspeaker' in var_list:\n",
    "                    title = 'isrefspeaker'\n",
    "                    val = 'shift'\n",
    "                    find = start['nar'].values[0]\n",
    "                    temp_ref.merge(self.find_val(title, val, temp_ref, ref_ment, find))\n",
    "                if 'deproleant' in var_list:\n",
    "                    title = 'deproleant'\n",
    "                    val = 'deprel'\n",
    "                    temp_ref.merge(self.get_val(title, val, temp_ref, ref_ment))\n",
    "                if 'deprolemen' in var_list:\n",
    "                    title = 'deprolemen'\n",
    "                    val = 'deprel'\n",
    "                    temp_ref.merge(self.get_val(title, val, temp_ref, start))\n",
    "                if 'deprolecomb' in var_list:\n",
    "                    title = 'deprolecomb'\n",
    "                    val = 'deprel'\n",
    "                    temp_ref.merge(self.comb_attr(title, val, temp_ref, start, ref_ment))\n",
    "                if 'samesent' in var_list:\n",
    "                    title = 'samesent'\n",
    "                    val = 'sid'\n",
    "                    temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "\n",
    "\n",
    "                ref_pos = ref_ment['udpos'].values[0]\n",
    "\n",
    "                if typ == 'noun': #noun specific vars\n",
    "                    if ref_pos in ('NOUN', 'PROPN'):\n",
    "                        if 'animacym' in var_list and ref_pos in ('NOUN', 'PROPN'):\n",
    "                            title = 'animacym'\n",
    "                            val = 'morph'\n",
    "                            temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                        if 'clustagree' in var_list: #cluster level info\n",
    "                            title = 'clustagree'\n",
    "                            val = 'morph'\n",
    "                            act = 'compare'\n",
    "                            temp_ref.merge(self.clust_info(title, val, temp_ref, start, ref_ment, deps, act))\n",
    "                        if 'containnum' in var_list: #cluster level info\n",
    "                            title = 'containnum'\n",
    "                            val = 'udpos'\n",
    "                            find = 'NUM'\n",
    "                            temp_ref.merge(self.find_val(title, val, temp_ref, ref_ment, find))\n",
    "\n",
    "\n",
    "                elif typ == 'ne': #Ne specific vars\n",
    "                    if ref_pos in ('NOUN', 'PROPN'):\n",
    "                        if 'animacym' in var_list:\n",
    "                            title = 'animacym'\n",
    "                            val = 'morph'\n",
    "                            temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                        if 'clustagree' in var_list: #cluster level info\n",
    "                            title = 'clustagree'\n",
    "                            val = 'morph'\n",
    "                            act = 'compare'\n",
    "                            temp_ref.merge(self.clust_info(title, val, temp_ref, start, ref_ment, deps, act))\n",
    "                    if 'NEtype_match' in var_list:\n",
    "                        title = 'NEtype_match'\n",
    "                        val = 'ne_type'\n",
    "                        temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment))\n",
    "                    if 'namepart' in var_list: #part of a name, not head. Ideally learns to not pair these.\n",
    "                        title = 'namepart'\n",
    "                        val = 'deprel'\n",
    "                        find = 'flat:name'\n",
    "                        temp_ref.merge(self.find_val(title, val, temp_ref, start, find))\n",
    "\n",
    "\n",
    "                elif typ == 'pron': # pronoun specific vars\n",
    "                    if 'person_match' in var_list:\n",
    "                        title = 'person_match'\n",
    "                        val = 'morph'\n",
    "                        temp_ref.merge(self.compare(title, val, temp_ref, start, ref_ment)) \n",
    "                    if 'ptypemen' in var_list:\n",
    "                        title = 'ptypemen'\n",
    "                        val = 'deprel'\n",
    "                        temp_ref.merge(self.get_val(title, val, temp_ref, start, start_ment))\n",
    "                        \n",
    "                temp_ref = temp_ref.fillna(0)\n",
    "                all_refs.append(temp_ref)\n",
    "        all_refdf = pd.concat(all_refs)\n",
    "        return all_refdf\n",
    "\n",
    "            \n",
    "    \n",
    "    def set_up_vars(self, docid, row, deps, df, answers, var_list, typ, ids, train):#add docid\n",
    "        '''\n",
    "        set up variables adn dfs for creating vars.\n",
    "        '''\n",
    "        if train == True:\n",
    "            start_refs = [deps[deps['shift'] == row.start_ref]] # can't be more than one, but program accepts lists here.\n",
    "            cor_ref = row.cor\n",
    "            incor_ref = row.incor\n",
    "            refs = [cor_ref, incor_ref]\n",
    "            tempdf = self.make_vars(deps, start_refs, refs, var_list, typ, train) # appends head ref pair variables\n",
    "            df = pd.concat([df, tempdf])\n",
    "            \n",
    "            for start_ref in start_refs:\n",
    "                iddf_cor = pd.DataFrame({'docid': [docid], 'start': [start_ref], 'ref':[cor_ref]})\n",
    "                iddf_incor = pd.DataFrame({'docid': [docid], 'start': [start_ref], 'ref':[incor_ref]})\n",
    "                ids = ids.append(iddf_cor)\n",
    "                ids = ids.append(iddf_incor)\n",
    "                answers.append(pd.DataFrame({'answer': [1]}))\n",
    "                answers.append(pd.DataFrame({'answer': [0]}))\n",
    "        elif train == False:\n",
    "            ser = row.series\n",
    "            if ser == True:#change morph for series to plural and split seperate mention for singular short ref.\n",
    "                start_ment = deps[deps['shift'] == row.start_ref]\n",
    "                start_ment_full = start_ment\n",
    "                start_ment_full['morph'] = self.sing_to_plur(start_ment['morph'].values[0].split('|'))\n",
    "                start_ment_part = start_ment\n",
    "                start_ment_part['full_men'] = start_ment_part['part_men']\n",
    "                start_ment_part['tk_shifts'] = start_ment_part['part_shifts']\n",
    "                start_refs = [start_ment_full, start_ment_part]\n",
    "            else:\n",
    "                start_refs = [deps[deps['shift'] == row.start_ref]]\n",
    "            refs = row.refs\n",
    "            tempdf = self.make_vars(deps, start_refs, refs, var_list, typ, train)\n",
    "            df = pd.concat([df, tempdf])\n",
    "            for start_ref in start_refs:\n",
    "                for ref in refs:\n",
    "                    iddf = pd.DataFrame({'docid': [docid], 'start': [start_ref], 'ref':[ref]})\n",
    "                    ids = ids.append(iddf)\n",
    "                               \n",
    "        return df, answers, ids\n",
    "\n",
    "\n",
    "                    \n",
    "                    \n",
    "    def start_vars(self, deps, answers, train, gold_var, no_gold_var, \\\n",
    "                   train_ids, idstr, ids_gold, \\\n",
    "                   ids_nogold, test_ids, train_var, gold, no_gold, type_all, docid, var_list):\n",
    "        '''\n",
    "        create mention variable lists for training and testing.\n",
    "        '''\n",
    "        \n",
    "        if docid in train_ids: #make training data.\n",
    "            print('train')\n",
    "            i = 1\n",
    "            for i, trow in train.iterrows():\n",
    "                train_var, answers, idstr = self.set_up_vars(docid, trow, deps, train_var, answers, \\\n",
    "                                                                        var_list, type_all, idstr, train=True)\n",
    "            \n",
    "            print(i)\n",
    "            i += 1\n",
    "            \n",
    "        elif docid in test_ids: # make test data\n",
    "            print('test')\n",
    "            no_answers = None\n",
    "            q = 1\n",
    "            for i, rowg in gold.iterrows():\n",
    "                gold_var_ne, no_answers, ids_gold = self.set_up_vars(docid, rowg, deps, gold_var, no_answers, \\\n",
    "                                                var_list, type_all, ids_gold, train=False)\n",
    "                print(q)\n",
    "                q += 1\n",
    "            \n",
    "            t = 1\n",
    "            for i, rowng in no_gold.iterrows():\n",
    "                no_gold_var, no_answers, ids_nogold = self.set_up_vars(docid, rowg, deps, no_gold_var, no_answers, \\\n",
    "                                                    var_list, type_all, ids_nogold, train=False)\n",
    "                print(t)\n",
    "                t += 1\n",
    "       \n",
    "        return answers, idstr, ids_gold, \\\n",
    "        ids_nogold, train_var, gold_var, no_gold_var, \n",
    "        \n",
    "        \n",
    "    def get_train_ids(self, train_per):\n",
    "        '''\n",
    "        randomizes training and test sets based on given percentage value, \n",
    "        returning the docids in two lists for training and testing.\n",
    "        '''\n",
    "        train_per = math.floor(train_per * 181)\n",
    "        train_ids = random.sample(range(1, 181), train_per) #harcoded quantity of texts.\n",
    "        test_ids = [x for x in range(1, 181) if x not in train_ids]\n",
    "        return train_ids, test_ids\n",
    "        \n",
    "    \n",
    "    def create_vars(self, var_list, type_all, train_per, create_men=True):\n",
    "        '''\n",
    "        pass through manual sieves, collect mention pairs, and create final variable list for all \n",
    "        training and test data sets.\n",
    "        '''\n",
    "        self.train_ids, self.test_ids = self.get_train_ids(train_per) #set ids fro training\n",
    "        for i, nrow in self.newdf.iterrows():\n",
    "            docname = nrow.docname\n",
    "            print(docname)\n",
    "            docid = nrow.docid\n",
    "            mens = pd.read_csv('coref/men_sieves/{0}'.format(nrow.docname), delimiter='\\t') # temp method for testing.\n",
    "            deps = pd.read_csv('coref/sieves/{0}'.format(nrow.docname), delimiter='\\t')\n",
    "            \n",
    "            if type_all == 'noun':\n",
    "                self.train_noun, self.gold_noun, \\\n",
    "                self.no_gold_noun = self.create_candidates(mens, deps, self.train_noun, self.gold_noun, self.no_gold_noun, \\\n",
    "                                                           docid, self.train_ids, self.test_ids, type_all)\n",
    "                \n",
    "                self.answers_noun, self.idstr_noun, self.ids_ne_gold, self.ids_ne_nogold, \\\n",
    "                self.train_var_noun, self.gold_var_noun, \\\n",
    "                self.no_gold_var_noun = self.start_vars(deps, self.answers_noun, \\\n",
    "                     self.train_noun, self.gold_var_noun, self.no_gold_var_noun, \\\n",
    "                     self.train_ids, self.idstr_noun, self.ids_noun_gold, \\\n",
    "                     self.ids_noun_nogold, self.test_ids, self.train_var_noun, \\\n",
    "                     self.gold_noun, self.no_gold_noun, type_all, docid, var_list)\n",
    "            \n",
    "            \n",
    "            elif type_all == 'ne':\n",
    "                self.train_ne, self.gold_ne, \\\n",
    "                self.no_gold_ne = self.create_candidates(mens, deps, self.train_ne, self.gold_ne, self.no_gold_ne, \\\n",
    "                                                           docid, self.train_ids, self.test_ids, type_all)\n",
    "            \n",
    "                self.answers_ne, self.idstr_ne, self.ids_ne_gold, self.ids_ne_nogold, \\\n",
    "                self.train_var_ne, self.gold_var_ne, \\\n",
    "                self.no_gold_var_ne = self.start_vars(deps, self.answers_ne, \\\n",
    "                     self.train_ne, self.gold_var_ne, self.no_gold_var_ne, \\\n",
    "                     self.train_ids, self.idstr_ne, self.ids_ne_gold, \\\n",
    "                     self.ids_ne_nogold, self.test_ids, self.train_var_ne, \\\n",
    "                     self.gold_ne, self.no_gold_ne, type_all, docid, var_list)\n",
    "                \n",
    "            elif type_all == 'pron':\n",
    "                self.train_pron, self.gold_pron, \\\n",
    "                self.no_gold_pron = self.create_candidates(mens, deps, self.train_pron, self.gold_pron, self.no_gold_pron, \\\n",
    "                                                           docid, self.train_ids, self.test_ids, type_all)\n",
    "\n",
    "                self.answers_pron, self.idstr_pron, self.ids_ne_gold, self.ids_ne_nogold, \\\n",
    "                self.train_var_pron, self.gold_var_pron, \\\n",
    "                self.no_gold_var_pron = self.start_vars(deps, self.answers_pron, \\\n",
    "                     self.train_pron, self.gold_var_pron, self.no_gold_var_pron, \\\n",
    "                     self.train_ids, self.idstr_pron, self.ids_pron_gold, \\\n",
    "                     self.ids_pron_nogold, self.test_ids, self.train_var_pron, \\\n",
    "                     self.gold_pron, self.no_gold_pron, type_all, docid, var_listt)\n",
    "            \n",
    "            \n",
    "            if not os.path.exists('coref/sets'): #for testing only\n",
    "                os.mkdir('coref/sets')\n",
    "\n",
    "            if type_all == 'ne':\n",
    "                self.train_ne.to_csv('coref/sets/train_ne', sep='\\t')\n",
    "                self.gold_ne.to_csv('coref/sets/gold_ne', sep='\\t') \n",
    "                self.no_gold_ne.to_csv('coref/sets/no_gold_ne', sep='\\t')\n",
    "                self.train_var_ne.to_csv('coref/train_var_ne', sep='\\t')\n",
    "                self.gold_var_ne.to_csv('coref/gold', sep='\\t')\n",
    "                self.no_gold_var_ne.to_csv('coref/nogold', sep='\\t')\n",
    "\n",
    "            elif type_all == 'noun':\n",
    "                self.train_noun.to_csv('coref/sets/train_noun', sep='\\t')\n",
    "                self.gold_noun.to_csv('coref/sets/gold_noun', sep='\\t')\n",
    "                self.no_gold_noun.to_csv('coref/sets/no_gold_noun', sep='\\t')\n",
    "                self.train_var_noun.to_csv('coref/train_var_noun', sep='\\t')\n",
    "                self.gold_var_noun.to_csv('coref/gold', sep='\\t')\n",
    "                self.no_gold_var_noun.to_csv('coref/nogold', sep='\\t')\n",
    "\n",
    "            elif type_all == 'pron':\n",
    "                self.train_pron.to_csv('coref/sets/train_pron', sep='\\t')\n",
    "                self.gold_pron.to_csv('coref/sets/gold_pron', sep='\\t')\n",
    "                self.no_gold_pron.to_csv('coref/sets/no_gold_pron', sep='\\t')\n",
    "                self.train_var_pron.to_csv('coref/train_var_pron', sep='\\t')\n",
    "                self.gold_var_pron.to_csv('coref/gold', sep='\\t')\n",
    "                self.no_gold_var_pron.to_csv('coref/nogold', sep='\\t')\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'newdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-380222c93194>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'genderm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'animacym'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'numberm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'headm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'partialheadm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentdist'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'nsubjant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'detant'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nsubjmen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'detmen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nar_match'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'antecedenttype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NEtype_match'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'person_match'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'namepart'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'clustagree'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'containnum'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'namepartref'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deprolemen'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'deproleant'\u001b[0m\u001b[0;34m,\u001b[0m            \u001b[0;34m'deprolecomb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'samesent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ptypemen'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexample2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreftrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mexample2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'noun'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_per\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'newdf' is not defined"
     ]
    }
   ],
   "source": [
    "var_list = ['genderm', 'animacym', 'numberm', 'headm', 'partialheadm', 'sentdist', \\\n",
    "           'nsubjant', 'detant', 'nsubjmen', 'detmen', 'nar_match', 'antecedenttype', 'NEtype_match', \\\n",
    "           'person_match', 'namepart', 'clustagree', 'containnum', 'namepartref', 'deprolemen', 'deproleant', \\\n",
    "           'deprolecomb', 'samesent','ptypemen']\n",
    "\n",
    "example2 = Coreftrain(newdf)\n",
    "\n",
    "example2.create_vars(var_list, 'noun', train_per=.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'gensim.models.keyedvectors' has no attribute 'load_word2vec_format'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-acfe6bafe84e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ruwikiruscorpora_0_300_20.bin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'gensim.models.keyedvectors' has no attribute 'load_word2vec_format'"
     ]
    }
   ],
   "source": [
    "model = gensim.models.keyedvectors.load_word2vec_format('ruwikiruscorpora_0_300_20.bin', binary=True, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7dd019feee6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWord2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ruwikiruscorpora_0_300_20.bin.gz'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "gensim.models.Word2Vec.load_word2vec_format('../ruwikiruscorpora_0_300_20.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'говорить' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-487-0aa88e061eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'говорить'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'сказать'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/john/diploma/env/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, w1, w2)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m         \"\"\"\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john/diploma/env/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/john/diploma/env/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'говорить' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "model.similarity('говорить', 'сказать')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mistakes in parser\n",
    "#воздушное пространство , и морскую акваторию ,\n",
    "# 121\t14.0\tгорный\tгорный\tADJ\t_\tCase=Nom|Degree=Pos|Gender=Masc|Number=Sing\t15.0\tamod\t_\t_\t6.0\t701.0\t6.0\t_\t_\t_\t_\n",
    "# 122\t15.0\tкластер\tкластер\tNOUN\t_\tAnimacy=Inan|Case=Nom|Gender=Masc|Number=Sing\t4.0\tconj\t_\t_\t7.0\t708.0\t6.0\tnoun\t_\tгорный кластер\t701, 708\n",
    "\n",
    "#mistakes in dependencies\n",
    "# 62\t62\t6.0\tсказал\tсказать\tVERB\t_\tAspect=Perf|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\t7\tparataxis\t_\t_\t6.0\t305.0\t9.0\t_\t_\t_\t_\t_\t_\t_\t_\n",
    "# 63\t63\t7.0\tчеловек\tчеловек\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t0\troot\t_\t_\t7.0\n",
    "\n",
    "# comma attached to вас makes imposible to parse for discourse.\n",
    "# 924\t924\t3.0\tблагодарю\tблагодарить\tNOUN\t_\tAnimacy=Inan|Case=Dat|Gender=Neut|Number=Sing\t1\tconj\t_\t_\t9.0\t4817.0\t66.0\tnoun\t_\tблагодарю\t4817\t_\t_\t_\t\n",
    "# 925\t925\t4.0\tвас,\tвас,\tPUNCT\t_\t_\t3\tpunct\t_\t_\t4.0\t4827.0\t66.0\t_\t_\t_\t_\t_\t_\t_\t\n",
    "# 926\t926\t1.0\t-\t-\tPUNCT\t_\t_\t2\tpunct\t_\t_\t1.0\t4831.0\t67.0\t_\t_\t_\t_\t_\t_\t_\t\n",
    "# 927\t927\t2.0\tспокойно\tспокойно\tADV\t_\tDegree=Pos\t3\tadvmod\t_\t_\t8.0\t4832.0\t67.0\t_\t_\t_\t_\t_\t_\t_\t\n",
    "# 928\t928\t3.0\tответил\tотвечать\tVERB\t_\tAspect=Perf|Gender=Masc|Mood=Ind|Number=Sing|Tense=Past|VerbForm=Fin|Voice=Act\t0\troot\t_\t_\t7.0\t4841.0\t67.0\t_\t_\t_\t_\t_\t_\t_\t\n",
    "# 929\t929\t4.0\tон,\tон,\tPUNCT\t_\t_\t3\tpunct\t_\t_\t3.0\t4849.0\t67.0\t_\t_\t_\n",
    "\n",
    "\n",
    "# trouble conecting female ending names with masc. nouns.\n",
    "# 120\t29.0\tсвоем\tсвой\tDET\t_\tCase=Loc|Gender=Masc|Number=Sing\t30\tamod\t_\t_\t5.0\t685.0\t8.0\trefl\tсвоем\t685\t_\t_\t_\t_\n",
    "# 121\t30.0\tвнуке\tвнук\tNOUN\t_\tAnimacy=Inan|Case=Loc|Gender=Masc|Number=Sing\t27\tobl\t_\t_\t5.0\t691.0\t8.0\tnoun\tсвоем внуке\t685, 691\t_\t_\t_\t_\n",
    "# 122\t31.0\tПорфишке\tПорфишка\tPROPN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t27\tobj\t_\t_\t8.0\t697.0\t8.0\tNE\tПорфишке\t697\t_\t_\t_\tPER\n",
    "\n",
    "\n",
    "# trouble separating sentences. Can't resolve который.\n",
    "# 989\t5.0\tавтор\tавтор\tNOUN\t_\tAnimacy=Anim|Case=Nom|Gender=Masc|Number=Sing\t0\troot\t_\t_\t5.0\t5623.0\t92.0\tnoun\t_\t_\t_\t_\tединственный русский автор XVIII в\t5602, 5615, 5623, 5629, 5635\n",
    "# 990\t6.0\tXVIII\txviii\tNUM\t_\t_\t7\tnummod\t_\t_\t5.0\t5629.0\t92.0\t_\t_\t_\t_\t_\t_\t_\n",
    "# 991\t7.0\tв\tвек\tNOUN\t_\tAnimacy=Inan|Case=Gen|Gender=Masc|Number=Sing\t5\tnmod\t_\t_\t1.0\t5635.0\t92.0\tnoun\t_\t_\t_\t_\tXVIII в\t5629, 5635\n",
    "# 992\t8.0\t.\t.\tPUNCT\t_\t_\t7\tpunct\t_\t_\t1.0\t5636.0\t92.0\t_\t_\t_\t_\t_\t_\t_\n",
    "# 993\t1.0\t,\t,\tPUNCT\t_\t_\t0\troot\t_\t_\t1.0\t5637.0\t93.0\t_\t_\t_\t_\t_\t_\t_\n",
    "# 994\t2.0\tкоторый\tкоторый\tPRON\t_\tCase=Nom\t3\tnsubj\t_\t_\t7.0\t5639.0\t93.0\trel\t_\t_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "To_do:\n",
    "#     1. finish merge and head match\n",
    "#     2. finish mention pos and neg extraction\n",
    "    3. train models for NE, common NP, pron (pron, poss, rel and refl)\n",
    "    4. finish sieves (priority on semantic and alias and set constuctions and then discourse)\n",
    "#         a. который - to head verb of clause, then grab noun head of verb. deprel verb = acl:relcl\n",
    "    5. create module for storing res (likely in same format as original)\n",
    "    6. create module for scoring\n",
    "    7. Run and test results of model\n",
    "    8. improve mention detection:\n",
    "#         a. deprel == case for secone cycle parts of mentions\n",
    "#         b. adj dependent on anything except nouns and verbs to be rel references or deprel == nsubj\n",
    "        c. dealing with chains and mentions in chains. # will probably be handled not in mention detection.\n",
    "        d. i within i relations\n",
    "#         e. remove end comma.\n",
    "        f. preprocessing for space between punctuation.\n",
    "        g. add other punctuation such as -\n",
    "#         h. deprels - nummod:entity\n",
    "        i. advmod but not pos PART #maybe. look into it.\n",
    "#         j. PROPN titles in mention\n",
    "        k. sentence splits on english words (foregin flat, root)\n",
    "        l. Make all categories from start and fix exceptions in program. (part_men, )\n",
    "        m. improve quotation module. (see 2.txt) \n",
    "    9. narrator combine in merge\n",
    "Cleaning list:\n",
    "    1. change number representations in dfs to single format\n",
    "    2. Improve speed by reducing redundant loops."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
